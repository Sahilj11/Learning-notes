## Intro
A queue is a collection designed to hold elements for processing, yielding them up in
the order in which they are to be processed. The corresponding Collections Framework
interface Queue (see Figure 14-1) has a number of different implementations

Many of the implementations use the
rule that tasks are to be processed in the order in which they were submitted (First In
First Out, or FIFO), but other rules are possible like storing data on basis of there priority

In addition to the operations inherited from Collection, the Queue interface includes
operations to add an element to the tail of the queue, to inspect the element at its head,
or to remove the element at its head. Each of these three operations comes in two
varieties, one which returns a value to indicate failure and one which throws an ex-
ception.


![[Pasted image 20231016111001.png]]

![[Pasted image 20231016111015.png]]

### Adding element to queue 
Although add does return a boolean signifying its success in inserting an element, that value can’t be used to report that a boun-
ded queue is full; the contract for add specifies that it may return false only if the
collection refused the element because it was already present—otherwise, it must throw
an exception.

The value-returning variant is offer:
```java
boolean offer (E e)
// insert the given element if possible
```
The value returned by offer indicates whether the element was successfully inserted
or not. Note that offer does throw an exception if the element is illegal in some way

### Retrieving element from queue
The methods in this group are peek and ele
ment for inspecting the head element, and poll and remove for removing it from the
queue and returning its value.
The methods that throw an exception for an empty queue are:
```java
E element()
E remove()
// retrieve but do not remove the head element
// retrieve and remove the head element
```
Notice that this is a different method from the Collection method remove(Object). The
methods that return null for an empty queue are:
```java
E peek()
E poll()
// retrieve but do not remove the head element
// retrieve and remove the head element
```
Because these methods return null to signify that the queue is empty, you should avoid
using null as a queue element. In general, the use of null as a queue element is dis-
couraged by the Queue interface, and the only standard implementation that allows it
is the legacy implementation LinkedList.

## Implementation of queue 
### Priority queue
PriorityQueue is one of the two nonlegacy Queue implementations not designed pri-
marily for concurrent use (the other one is ArrayDeque)
#### What is concurrent use
In the context of data structures and programming, "concurrent use" refers to the ability of multiple threads or processes to access and manipulate the data structure simultaneously. When multiple threads or processes can operate on a data structure concurrently, it introduces the potential for race conditions and other concurrency-related issues, which can lead to unpredictable and often undesirable behavior in your program.

Java's `PriorityQueue` is one of the two non-legacy Queue implementations that are not explicitly designed for concurrent use. This means that `PriorityQueue` is not thread-safe by default. Thread-safety in this context means that the data structure is not protected against concurrent access and modification by multiple threads. When multiple threads are involved, you need to implement your own synchronization mechanisms (e.g., using locks or other concurrency control tools) to ensure that the `PriorityQueue` is accessed and modified safely.

If you need to use a queue in a multi-threaded environment, Java provides a few concurrent queue implementations like `ConcurrentLinkedQueue`, `ConcurrentLinkedDeque`, and `LinkedBlockingQueue`, among others. These classes are explicitly designed to handle concurrent access, and they provide built-in synchronization mechanisms to ensure that multiple threads can safely use the queue without causing data corruption or race conditions.

#### How PQ handle equals
PriorityQueue gives no guarantee of how it presents multiple elements with the same value. So if, in our example, several tasks are tied for the highest priority in the queue, it will
choose one of them arbitrarily as the head element.

#### Constructor 
```java
PriorityQueue()
// natural ordering, default initial capacity (11)
PriorityQueue(Collection<? extends E> c)
// natural ordering of elements taken from c, unless
// c is a PriorityQueue or SortedSet, in which case
// copy c's ordering
PriorityQueue(int initialCapacity)
// natural ordering, specified initial capacity
PriorityQueue(int initialCapacity, Comparator<? super E> comparator)
// Comparator ordering, specified initial capacity
PriorityQueue(PriorityQueue<? extends E> c)
// ordering and elements copied from c
PriorityQueue(SortedSet<? extends E> c)
// ordering and elements copied from c
```

#### Implementing priority queue using binary tree and how it is different from treeset
Priority queues are usually efficiently implemented by priority heaps. A priority heap is
a binary tree somewhat like those we saw implementing TreeSet in Section 13.2.2, but
with two differences: first, the only ordering constraint is that each node in the tree
should be larger than either of its children, and second, that the tree should be complete
at every level except possibly the lowest; if the lowest level is incomplete, the nodes it
contains must be grouped together at the left
[[1.3. Bags , queue and stacks#heapSort]]

### ConcurrentLinkedQueue
We noticed there that one of the main attractions of linked struc-
tures is that the insertion and removal operations implemented by pointer rearrange-
ments perform in constant time. This makes them especially useful as queue imple-
mentations, where these operations are always required on cells at the ends of the
structure—that is, cells that do not need to be located using the slow sequential search
of linked structures.

ConcurrentLinkedQueue uses a CAS-based wait-free algorithm—that is, one that guar-
antees that any thread can always complete its current operation, regardless of the state
of other threads accessing the queue
#### CAS wait-free algo
A CAS-based wait-free algorithm (Compare-And-Swap-based wait-free algorithm) is a synchronization technique used in concurrent and parallel programming to ensure that multiple threads or processes can access shared data structures without any thread being forced to wait indefinitely for its turn to access the data. Wait-freedom is a strong form of progress guarantee in concurrent programming, which means that every thread will make progress within a bounded number of steps, regardless of the contention or interference from other threads.

The central operation in a CAS-based wait-free algorithm is the Compare-And-Swap (CAS) operation, which is typically provided by modern processors and is an atomic operation. CAS allows a thread to read a value from a memory location, compare it to an expected value, and, if they match, update the memory location with a new value. The key property of CAS is that it's atomic, meaning that no other thread can interfere or modify the memory location between the read and the write if the comparison succeeds.

Here's a high-level explanation of how a CAS-based wait-free algorithm works:

1. Each thread or process tries to perform a specific task concurrently with other threads.

2. Before modifying a shared data structure, a thread reads the current value using a CAS operation and keeps the value as the "expected" value.

3. The thread then performs the necessary computation to determine the new value to be written to the shared data structure.

4. The thread attempts to update the shared data structure using CAS. If the CAS succeeds (i.e., the current value matches the expected value), the update is performed. If the CAS fails, indicating that another thread has modified the data in the meantime, the thread will retry the operation.

5. If a thread's CAS operation fails repeatedly, it will continue to retry until it succeeds. This ensures that every thread will eventually make progress.

6. Since CAS operations are atomic, other threads will not be blocked or delayed while one thread is retrying its CAS operation.

The key idea behind wait-freedom is that even in the presence of contention and multiple threads competing for the same resource, each thread is guaranteed to make progress within a bounded number of steps, ensuring that no thread gets stuck or starved. This is a desirable property in real-time systems, where predictable and guaranteed performance is critical.

Wait-free algorithms can be quite challenging to design and implement, as they require careful synchronization and often lead to a high number of retries when there is contention. However, they provide strong guarantees about the progress of each thread and are valuable in situations where determinism and predictable performance are crucial.
### BlockingQueue(Subinterface of Queue)
![[Pasted image 20231016131024.png]]
One common example of the use of producer-consumer queues is in systems that per-
form print spooling; client processes add print jobs to the spool queue, to be processed
by one or more print service processes, each of which repeatedly “consumes” the task
at the head of the queue.

The key facilities that BlockingQueue provides to such systems are, as its name implies,
enqueuing and dequeueing methods that do not return until they have executed suc-
cessfully. So, for example, a print server does not need to constantly poll the queue to
discover whether any print jobs are waiting; it need only call the poll method, supplying
a timeout, and the system will suspend it until either a queue element becomes available
or the timeout expires.

#### Methods 
##### Adding element 
```java
boolean offer(E e, long timeout, TimeUnit unit)
// insert e, waiting up to the timeout
void put(E e)
// add e, waiting as long as necessary
```

##### Removing element 
```java
E poll(long timeout, TimeUnit unit)
// retrieve and remove the head, waiting up to the timeout
E take()
// retrieve and remove the head of this queue, waiting
// as long as necessary
```

##### Retrieving or querying the content 
```java
int drainTo(Collection<? super E> c)
// clear the queue into c
int drainTo(Collection<? super E> c, int maxElements)
// clear at most the specified number of elements into c
int remainingCapacity()
// return the number of elements that would be accepted
// without blocking, or Integer.MAX_VALUE if unbounded
```

#### Implementation of BQ
##### LinkedBlockingQueue
This class is a thread-safe, FIFO-ordered queue, based on a linked node structure. It is
the implementation of choice whenever you need an unbounded blocking queue
```
What does unbounded blocking queue means?

In the context of the sentence you provided, "unbounded" means that there is no predefined limit or maximum capacity to the number of elements that can be added to the queue. An unbounded queue can theoretically hold an unlimited number of items, or at least a number of items that is only limited by available memory or system constraints.

So, when it says "you need an unbounded blocking queue," it's referring to a queue structure where you can keep adding elements without running into a fixed capacity or limit, and the queue will dynamically resize itself as needed to accommodate new elements. This can be contrasted with bounded queues, which have a predefined maximum capacity, and adding elements beyond that capacity would result in some form of blocking or other specified behavior.
```

##### ArrayBlockingQueue
This implementation is based on a circular array—a linear structure in which the first
and last elements are logically adjacent

A circular array in which the head and tail can be continuously advanced in this way
this is better as a queue implementation than a noncircular one (e.g., the standard
implementation of ArrayList, which we cover in Section 15.2) in which removing the
head element requires changing the position of all the remaining elements so that the
new head is at position 0

**circular array**
```java
public class queue{
    public static void main(String[] args) {
        String[] arr = {"A","B","C","D"};
        int length = arr.length;
        // circular_index = index % length_of_array
        for (int index = 0; index < length; index++) {
           System.out.println(arr[index % length]); 
        }
    }
}
```

`ArrayBlockingQueue` is typically implemented using a circular array to efficiently manage its internal data structure. Here's how the circular array is used and why it's beneficial:

1. **Circular Array Structure**:
   - Instead of using a standard linear array, `ArrayBlockingQueue` employs a circular array as its underlying data structure. This circular arrangement allows for efficient use of memory and provides the required properties for a blocking queue.

2. **Circular Indexing**:
   - Circular indexing means that when an element is added or removed from the queue, the index used to access the array "wraps around" when it reaches the end of the array. This is done using modulo arithmetic. For example, if the array size is 10 and you add an element at index 9, the next addition will wrap around to index 0.

3. **Efficient Enqueue and Dequeue**:
   - The circular array structure ensures that enqueue (addition) and dequeue (removal) operations are efficient, regardless of the number of elements in the queue. These operations have constant time complexity, O(1), since they only involve index manipulation and not shifting or copying of elements.

4. **Optimal Memory Usage**:
   - Circular arrays are memory-efficient because they allow for a fixed amount of memory to be used efficiently. Elements can be stored sequentially, and when the array reaches capacity, new elements simply overwrite the oldest elements in a cyclical manner. This helps prevent unnecessary memory allocation and deallocation.

5. **Wrap-Around Logic**:
   - The circular array structure is managed through wrap-around logic. When adding 
   - elements, the "tail" of the queue advances circularly within the array. When removing elements, the "head" of the queue advances in a similar manner. The use of mod operations is crucial in ensuring that these indices remain within the bounds of the array.

6. **Implementation Benefits**:
   - Using a circular array simplifies the implementation of a bounded blocking queue like `ArrayBlockingQueue`. The circular structure eliminates the need for complex resizing operations, as elements are continuously recycled within the same memory space.

In summary, the circular array structure in `ArrayBlockingQueue` is crucial for its efficiency and performance. It allows for constant-time enqueue and dequeue operations, efficient memory usage, and straightforward implementation of a bounded blocking queue. This circular array concept is a key factor in the success of `ArrayBlockingQueue` for managing concurrent data in Java applications.

**Info about arrayblockingqueue**
`ArrayBlockingQueue` is a commonly used implementation of the `BlockingQueue` interface in Java. It provides a bounded, thread-safe queue for managing data in a multi-threaded environment. Below are some key points about `ArrayBlockingQueue`:

1. **Bounded Queue**: `ArrayBlockingQueue` has a fixed capacity specified when you create an instance. Once the queue is full, any attempt to add more elements will block until space becomes available.

2. **FIFO Order**: Elements in the `ArrayBlockingQueue` are stored and retrieved in a first-in, first-out (FIFO) order. This means that the elements are processed in the order they were added.

3. **Thread-Safe**: It is designed for use in multi-threaded applications. It provides the necessary synchronization to ensure that multiple threads can safely add and remove elements without running into data corruption or race conditions.

4. **Blocking Operations**: `ArrayBlockingQueue` provides blocking operations for adding and removing elements, which are particularly useful in producer-consumer scenarios. For instance, if you try to remove an element from an empty queue, the operation will block until an element is available.

5. **Blocking on Insertion**: When trying to add elements to a full queue, the `put()` method will block until there is space. Alternatively, you can use `offer()` to attempt insertion without blocking and checking if it succeeded, or `offer()` with a timeout to wait for a limited amount of time.

6. **Blocking on Retrieval**: When trying to remove elements from an empty queue, the `take()` method will block until an element becomes available. The `poll()` method can be used for non-blocking retrieval.

7. **Fairness**: You can create a fair `ArrayBlockingQueue` where the order of access is based on the order in which threads request access to the queue. This can help prevent certain threads from being starved in high contention scenarios.

8. **Performance**: `ArrayBlockingQueue` is generally faster and more memory-efficient than other blocking queue implementations (such as `LinkedBlockingQueue`) because it uses an array to store elements.

9. **Not Suitable for Dynamic Sizing**: As an array-backed queue, `ArrayBlockingQueue` is not well-suited for scenarios where you need to dynamically resize the queue. If you require a dynamically resizing queue, consider using `LinkedBlockingQueue` or `ConcurrentLinkedQueue`.

In summary, `ArrayBlockingQueue` is a powerful tool for managing concurrent data in Java applications, especially in scenarios where you need to control the maximum capacity and ensure thread-safety. It is particularly useful in cases like producer-consumer problems or scenarios where you want to control and limit the number of elements in the queue.
##### PriorityBlockingQueue

This implementation is a thread-safe, blocking version of PriorityQueue (see Sec-
tion 14.2), with similar ordering and performance characteristics. Its iterators are fail-
fast, so in normal use they will throw ConcurrentModificationException; only if the
queue is quiescent will they succeed. To iterate safely over a PriorityBlockingQueue,
transfer the elements to an array and iterate over that instead.

## Deque
A deque (pronounced “deck”) is a double-ended queue. 
a deque can accept elements for insertion and present them for inspection or removal at
either end. Also unlike Queue, Deque’s contract specifies the ordering it will use in pre-
senting its elements: it is a linear structure in which elements added at the tail are yielded
up in the same order at the head. Used as a queue, then, a Deque is always a FIFO
structure; the contract does not allow for, say, priority deques. If elements are removed
from the same end (either head or tail) at which they were added, a Deque acts as a stack
or LIFO (Last In First Out) structure.

### Order in deque 
The quote you provided is explaining a key difference between a `Queue` and a `Deque` in terms of their contracts and element ordering.

1. **Queue**:
   - A `Queue` is a data structure that follows the basic FIFO (First-In-First-Out) principle. Elements are added to the back of the queue (enqueue) and removed from the front (dequeue). In a regular `Queue`, there is no specific guarantee about the order in which elements are presented or yielded when dequeued. The order depends on the internal implementation and may not necessarily reflect the order in which elements were added.

2. **Deque**:
   - A `Deque`, which stands for "double-ended queue," extends the concept of a `Queue` by allowing elements to be added or removed from both ends. The key point in the provided quote is that the contract of a `Deque` specifies a particular ordering for its elements.
   - In a `Deque`, elements that are added at the tail (the back) are guaranteed to be yielded up in the same order at the head (the front). This means that if you add elements A, B, and C to the back of a `Deque` in that order, and you start dequeuing from the front, you will retrieve A, then B, and then C in the same order in which they were added.

In simpler terms, a `Deque` ensures that it maintains the order of elements in a linear structure, much like a regular `Queue`, but with the added capability of allowing you to manipulate elements from both ends. This guarantees that when you dequeue elements from the front of the `Deque`, they are presented in the order in which they were added.

This ordered behavior makes `Deque` particularly useful in scenarios where you need to maintain a specific sequence of elements and perform operations at both ends of the structure while preserving that sequence.

### Methods 
```java
Collection-like Methods
void addFirst(E e)
// insert e at the head if there is enough space
void addLast(E e)
// insert e at the tail if there is enough space
void push(E e)
// insert e at the head if there is enough space
boolean removeFirstOccurrence(Object o);
// remove the first occurrence of o
boolean removeLastOccurrence(Object o);
// remove the last occurrence of o
Iterator<E> descendingIterator()
// get an iterator, returning deque elements in
// reverse order
```

#### Queue like methods 
```java
boolean offerFirst(E e) // insert e at the head if the deque has space
boolean offerLast(E e) // insert e at the tail if the deque has space
```

```java
E peekFirst()
E peekLast()
E pollFirst()
E pollLast()
// retrieve but do not remove the first element
// retrieve but do not remove the last element
// retrieve and remove the first element
// retrieve and remove the last element
```
The methods peekFirst and pollFirst are renamings of the equivalent methods peek
and poll on the Queue interface.
The methods that throw an exception for an empty deque are:
```java
E getFirst()
E getLast()
E removeFirst()
E removeLast()
E pop()
// retrieve but do not remove the first element
// retrieve but do not remove the last element
// retrieve and remove the first element
// retrieve and remove the last element
// retrieve and remove the first element
```

### Implementation of deque
#### ArrayDeque
ArrayDeque is now the general-purpose implementation of choice, for both deques and FIFO queues. It has the performance characteristics of a circular array: adding or removing elements at the head or tail takes constant time. The iterators are fail-fast.

#### BlockingDeque
implementations : - LinkedBlockingDeque

## Summary
In choosing a Queue, the first question to ask is whether the implementation you choose
needs to support concurrent access; if not, your choice is straightforward. For FIFO
ordering, choose ArrayDeque; for priority ordering, PriorityQueue

If your application does demand thread safety, you next need to consider ordering. If
you need priority or delay ordering, the choice obviously must be PriorityBlocking
Queue or DelayQueue, respectively. If, on the other hand, FIFO ordering is acceptable,
the third

![[Pasted image 20231016140200.png]]
