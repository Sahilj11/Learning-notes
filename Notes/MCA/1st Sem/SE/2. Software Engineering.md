## Data modeling 
Data modeling is the process of creating a conceptual representation of data and its relationships in order to understand and manage data effectively. It is a crucial step in database design and information system development. Data modeling helps in organizing and structuring data so that it can be stored, accessed, and analyzed efficiently

if application is data intensive then ER model can be used to produce conceptual data model of system

ER diagram servers as a documentation tool
### Obj of data modelling 
- to identify the data items in application and relationship b/w those identified data items 
- to create conceptual schema for the data.

#### ER Model
- IEEE defines "diagram that depicts the set of real world entities and logical relationship among them"
- Basic units 
	- Entities
	- type of entities
	- relationship 
	- Attributes
**Already covered in DBMS**

## Structural analysis 
### What is it 
- top-down approach in which focus is on refining the problem with the help of function performed in the problem domain and data produced by system.
### Principles of SA
- Top-down decomposition approach
- application of divide and conquer principle :- each high level function is independently decomposed into detailed function

### Objective of SA
- To provide graphical representation to develop new software or to enhance the existing software

### Tools for structural analysis
#### Data flow diagram
##### Intro
- diagram that depicts data sources , data sinks , data storage , processes performed on data as nodes and logical flow of data as the links b/w the nodes
- it is also called Data flow graph / Bubble Chart/ Bubble Graph
##### DFD vs Flow Chart 
- DFD should not be confused with flow chart . DFD represent the flow of data whereas flow chart represent flow of control/logic 

##### Obj of DFD
- Represent the system data in hierarchical order with req lvl of details
- Depicts the processes according to the defined user req and scope of software 
- To provide the overview of transformation that occur in the input data within the system to produce the output
- DFD depicts the flow of data within the system and considers a system that transform the input into req outputs 
- WHen the system is complex then data needs to be transformed by various steps to produce the output . These steps are required to refine the requirement 
##### +ve of DFD
- Provides the functional decomposition of proposed system
- provides the details of data flow in proposed system
- communicates the existing system knowledge to stakeholder
##### Units of DFD
- DFD shows the flow of data through a system . it views the system as function that transforms the input into desired outputs . any complex system does not perform the transformation in the single step but undergoes into number of transformation before it becomes the output
- Process:- 
	- The agent that performs the transformation from one state to another is called a process or bubble . Process is represented by named/ labeled circle.
- Data flow:-
	- Data flow is represented by named / labeled arrow
- Data store:- 
	- Data store is represented by open rectangle . Open rectangle should be named or labeled 
- Source or sink / External entity:- Source (originator of data) and sink(consumer of data) is represented by labeled rectangle 
- ![[Pasted image 20231008151125.png]]

##### Guidelines for creating DFD
- Meaninful names to be given to DFD notation . eg . in general the verbs should be used for naming a process where nouns should be used for the external entity , data flow and data store
- DFD should be created in organised order so that it can be easily understood
- Each process should be numbered uniquely 
- All the symbols used in DFD should be labeled

##### Synchronous and Asynchronous Operations of DFD
- ![[Pasted image 20231008151757.png]]

##### Developing DFD model of system
- DFD shows the flow of data through system . it views the system as function that transforms input into desired outputs
**LEVELED DATA FLOW DIAGRAMS**
- Any complex system does not perform the transformation in single step but undergoes into the number of transformation before it becomes the output
- Context diagram :- context diagram is most abstract data flow 
	- it is the most abstract data flow representation of system. it represent the entire system as a single bubble . In fig the banking system is represented as single process . Process banking system is represented by circle . User is the source and user is sink . Source and sink are represented by rectangle . Data flow represented by arrow
	- To develop the context diagram of system is to be analysed to identify the different type users who would be using system and the kinds of data they would be inputting to the system and the kinds of data they would be receiving from the system . Context lvl DFD is called as 0-lvl DFD
	- ![[Pasted image 20231008152638.png]]
	- 
- LVL 1- DFD :- Context lvl DFD is decomposed into LVL1 - DFD
	- User entity (source) is related to several processes in the bank which includes register. User support and provide cash . The transaction can be performed if the user is already registered in the bank
	- once user is registered . he can perform the transaction by the processes namely Deposit cheque , deposit cash and withdraw cash
	- The line in the process symbol indicates the level of process and contains a unique identifier in the form of number 
	- The user info such as name , address and account number is stored in data store User_detail
	- It is not necessary that the user is registered in the bank to have the demand draft then the details of the cash and date are stored in the data store , DD_detail
	- Once the demand draft is prepared then its receipt is provided to the user (sink)
	- ![[Pasted image 20231008153505.png]]
- LVL2 - DFD :- Processes of LVL 1 is decomposed into LVL 2
	- ![[Pasted image 20231008153536.png]]
	- 
- LVL 3 - DFD :- Processs of LVL 2 is decomposed into LVL 3
	- ![[Pasted image 20231008153607.png]]
- this process of decomposition is up to N level and it depends upon the details required in DFD

##### Informal verfication of DFD and data dictionary 
- After construction of DFD , an analyst can perform the walkthrough and look for error 
	- Unlabelled data flows 
	- Missing data flow 
	- Missing processes 
	- Consistency not maintained during the refinement
### Steps of SA
- study the physical environment of system
	- During this step DFD of current non automated system is drawn which shows the input and output data flows 
	- How the data flows through the system ?
	- what are the different processs operating on data 
	- While drawing the DFD for physical env the analyst have to interact with the users to determine the overall process from the point of view of data 
	- In this step context diagram is created in which the entire system is treated as a single process . in the context diagram all the inputs to the process , outputs from process , sources and sinks are identified and drawn
- Decomposition of process
	- Based on top down refiendment the context lvl process is decomposed into diff levels
- Draw logical equivalents of DFD for physical system
	- During this step the DFD physical env is taken and all the specific data flows are represented by their logical equivalents . the physical bubbles may be replaced by the logical processes . bubbles that do not transforms data in any form may be deleted from DFD
- Modeling the current system:- During the first two steps the current system is modeled and the next step is to model the new system after the changes have been incorporated . what are data flows and what are the major processes in this new system must be determined by analyst based upon his experience and vision
- Establishing the man machine boundary:- The next step is to establish the man machine boundary by specifying that what needs to automated and what will remain manual in new system.

## Data dictionary 
### Intro
- When system analysis is performed then it is essential for the system analyst to decide the necessary data for designing the application 
- DFD gives the info about the data flows and data stores of system so the individual data item of data flows and data stores can be catalogued . The catalogue can bring out that if any data is missing or duplicated . Such a catalogue is called data dictionary or data repository
- DD is essential tool in SDP to describe the DD sufficient info should be collected about the data items
- DD is a data repository which stores the info about system data at one place
- The DD is a reference work of data about data (metadata)
### Format of DD
![[Pasted image 20231008160749.png]]
![[Pasted image 20231008160848.png]]
![[Pasted image 20231008160906.png]]
### Others 
- DD should contains info about the following 
	- Data flows
		- ![[Pasted image 20231008161205.png]]
		- 
		- Is described by means of DFD . these are path along which data travels and data stores and places where data is stored. it can be said that data flow are data structure in motion and data stores are data structure in rest
	- Data stores
		- ![[Pasted image 20231008161225.png]]
	- Processes
		- ![[Pasted image 20231008161249.png]]
	- External Entities
		- ![[Pasted image 20231008161306.png]]
### Data Dictionary classes
- Data element :- it is the smallest unit of data and is also called as data item. each data element should be properly documented in data dictionary
- Data structure:- Once data items are defined then DS req by particular system under consideration can be identified . Data elements are combined to form DS
- ![[Pasted image 20231008161524.png]]
### use of DD
- important documentation which would be useful to maintain the system
- purpose of DD is to provide a source of reference for analyst , user and designer
- Analyst use the DD for collecting , documenting and for organising the specific facts about system
- It eliminates the redundancy 
- helps in validation of DFD
- becomes the starting point for developing the screen reports and forms

## Structure chart 
### What is it 
- Graphic depiction of decomposition of problem . Structure chart illustrate the partitioning of modules into sub modules . SC are used to show the communication b/w modules
- show the control and dataflow b/w modules
- SC represent the hierarchical structure of modul
- helps the programmer to divide the problems into smaller parts so that problem can be easily understood 
- it is used as tool to help in software design
- in design stage the structure chart is drawn and is used by clients and by designer to communicate b.w them
### Components of SC
- Modules 
- Connection b/w modules
- Communication b/w modules
### Basic building block of SC
![[Pasted image 20231008162330.png]]
### Modules
- ![[Pasted image 20231008162541.png]]
- types
	- Control module: Module at top in figure is called as root module also called control module
	- Subordinate module:- Modules can be subordinate module . In fig module i.e m1 , m2 are subordinate modules and these are also represented by rectangles
	- Library modules:- usually represented by rectangle with double edge . comprises the frequently called modules. when the module is invoked by many other module then it is made into library module 
	- ![[Pasted image 20231008162815.png]]
### Arrow 
![[Pasted image 20231008162912.png]]
### Data couple
![[Pasted image 20231008162946.png]]

### Control couple
![[Pasted image 20231008163022.png]]

### Diamond box
- it is used for indicating decision and depending upon decision one of the sub modules can be invoked
- In figure it is shown that module Print valid customer No . takes the decision based on the flag (control couple) valid customer no . whether to print a customer no. or to print an error message . Based on the decision the relevant module can be invoked ie. if customer no is valid then the print customer no . module is invoked otherwise module print error message is invoked
- ![[Pasted image 20231014153943.png]]

### Arc with arrow head 
- represent iteration
- a loop around control flow arrows denotes that the respective modules are invoked repeatedly 
- The number of modules that come within the loop is shown by number of arrows invoking modules
- In fig it is shown that three sub modules . Get customer no, validate customer no and print customer no are invoked repeatedly by the print valid customer no.
- ![[Pasted image 20231014154157.png]]

### Layered approach
- In any structure chart there should be one and only one module at the top called root
- There should be at most one control relationship b/w any two modules in the structure chart i.e if a module A is invokes module B then module B cannot invoke module A
- The layered design and poorly layered design is shown in fig
- ![[Pasted image 20231014154407.png]]

### Flow Chart vs Structure chart
- flow chart represent the flow of control/logic but it is difficult to identify the different modules of program from its flow . Structure chart identify the different modules of software
- The flow charts do not show the data exchange b/w different module but structure chart represent data exchange b/w modules
### Steps to create structure chart
1. First depict relation of modules from top to bottom
2. Conceptualise the main sub tasks that must be performed by program to solve the problem
3. Focus is given on each subtask and conceptualised that how each sub task can be broken into smaller tasks
4. Once the Strucutred chart is designed then the bottom up implementation of each module 
![[Pasted image 20231014154842.png]]

![[Pasted image 20231014154913.png]]
![[Pasted image 20231014154939.png]]
![[Pasted image 20231014155024.png]]

## Object oriented methodologies
### What is modeling 
- It is an abstraction of something for the purpose of understanding it before building it 
- Because real system that we want to study are generally very complex . In order to understand the real system , we have to simplify the system
- So a model is an abstraction that hides the non-essential characteristics of a system and highlights those features, wihch are pertinent to understand it 
- Most of modelling techq , for the analysis and design used graphics so the symbols are used acc to certain set of rules of methodology for communicating the complex relationship of information more clearly than the descriptive text
- need
	- To test physical entity before actual building it 
	- To set the basics for communication b/w the client and developer 
	- For finding the different alternatives
	- For reducing the complexity in order to understand the system
### Object oriented methodologies
![[Pasted image 20231014162738.png]]
#### Steps of OOP
![[Pasted image 20231014162806.png]]

### Structure vs OOP
![[Pasted image 20231014162846.png]]

### +ves of OOM
![[Pasted image 20231014162914.png]]

### Type of OOM
![[Pasted image 20231014162942.png]]
![[Pasted image 20231014163008.png]]
![[Pasted image 20231014163029.png]]
![[Pasted image 20231014163057.png]]
![[Pasted image 20231014163120.png]]

### What is UML
![[Pasted image 20231014163153.png]]


### Object Modelling Techniques
![[Pasted image 20231014163257.png]]
![[Pasted image 20231014163325.png]]
![[Pasted image 20231014163346.png]]
![[Pasted image 20231014163411.png]]
![[Pasted image 20231014163433.png]]
![[Pasted image 20231014163504.png]]
![[Pasted image 20231014163522.png]]
![[Pasted image 20231014163544.png]]
![[Pasted image 20231014163600.png]]
![[Pasted image 20231014163622.png]]
![[Pasted image 20231014163720.png]]
![[Pasted image 20231014163744.png]]
![[Pasted image 20231014163802.png]]
![[Pasted image 20231014163825.png]]
![[Pasted image 20231014163855.png]]
![[Pasted image 20231014163926.png]]
![[Pasted image 20231014163944.png]]
![[Pasted image 20231014164001.png]]

## Coding Standards 
### ISO 9126
ISO 9126 is indeed a recognized international standard. ISO/IEC 9126, titled "Software engineering - Product quality," is a standard that defines a framework for the evaluation of software product quality. It provides a set of quality characteristics and sub-characteristics that can be used to assess the quality of software. The standard is often used by software developers, quality assurance professionals, and other stakeholders to evaluate and improve the quality of software products.

1. **Functionality:**
   - **Suitability:** This sub-characteristic assesses the software's capability to meet specific needs and requirements. It considers how well the software satisfies the intended functions and features.
   - **Accuracy:** Accuracy evaluates the precision and correctness of the software's output. It's about ensuring that the software produces accurate results.
   - **Security:** Security measures the software's ability to protect data and functionalities from unauthorized access, ensuring that it is robust against security threats.
   - **Interoperability:** Interoperability checks whether the software can interact and work effectively with other systems, ensuring compatibility and seamless communication.

2. **Reliability:**
   - **Fault Tolerance:** Fault tolerance evaluates the software's ability to continue functioning correctly even in the presence of faults or errors. It measures the system's resilience to failures.
   - **Maturity:** Maturity assesses how well the software has evolved and improved over time. It implies that a mature software is more reliable and stable.
   - **Recoverability:** Recoverability refers to the software's ability to recover from failures or errors, ensuring that it can return to a working state after encountering issues.

3. **Usability:**
   - Usability examines how user-friendly the software is. It focuses on factors like ease of use, intuitiveness, and overall user satisfaction.

4. **Efficiency:**
   - **Time Behavior:** Time behavior assesses how quickly the software performs its tasks, including response times and processing speed.
   - **Resource Behavior:** Resource behavior evaluates how efficiently the software utilizes system resources, such as CPU, memory, and network bandwidth. Efficient software should use minimal resources.

5. **Maintainability:**
   - **Testability:** Testability measures how easily the software can be tested. It includes aspects like whether it's straightforward to create and run tests to verify its functionality and quality.
   - **Changeability:** Changeability assesses how easily the software can be modified or updated. It includes factors like code maintainability and how well it accommodates changes without introducing new errors.
   - **Analyzability:** Analyzability is about how easy it is to understand and analyze the software's structure and code to identify and diagnose issues.
   - **Stability:** Stability evaluates the software's ability to maintain its quality and performance even after modifications or updates. It should not become less reliable or efficient when changes are made.

6. **Portability:**
   - **Installability:** Installability measures how easily the software can be installed and set up on different platforms and environments.
   - **Adaptability:** Adaptability assesses how well the software can adapt to different operating environments and configurations.
   - **Replaceability:** Replaceability evaluates how easily the software can be replaced by another system or component without causing disruption to the overall system.

These quality characteristics and sub-characteristics provide a structured way to assess and evaluate software quality, ensuring that software meets user expectations, performs reliably, and can be maintained and adapted effectively.
ISO/IEC 9126 helps organizations and software developers to set quality objectives, make informed decisions about software development and maintenance, and communicate quality requirements to all stakeholders.
### Software vs Hardware relability 
![[Pasted image 20231021112959.png]]

**Software Reliability:**

Software reliability refers to the probability that a software system will perform its intended functions without failures over a specified period of time and under certain conditions. It is a measure of the software's ability to operate correctly and consistently. Software reliability is related to how well the software behaves under various circumstances, and it involves factors like error rates, availability, and mean time between failures (MTBF).

**Hardware Reliability:**

Hardware reliability, on the other hand, focuses on the reliability of physical components and devices within a computer system. It assesses the probability that hardware components (e.g., processors, memory, storage devices) will operate without failures over a specific duration and under specific environmental conditions. Hardware reliability often involves metrics like mean time between failures (MTBF), mean time to repair (MTTR), and failure rates.

**Software Reliability Curve:**
![[Pasted image 20231021111836.png]]
The software reliability curve represents the relationship between the cumulative number of defects or failures and time. It typically starts high and decreases as the software is tested and used. The shape of the curve can vary depending on the software and its development and testing process. There are two key phases in the software reliability curve:

1. **Infant Mortality Phase:** In this phase, which occurs early in the software's life cycle, the number of defects or failures is relatively high. This is because initial testing and usage reveal many issues, and they are fixed as they are discovered. As a result, the number of defects decreases rapidly during this phase.

2. **Steady-State Phase:** In the steady-state phase, the software's reliability stabilizes, and the rate of defect discovery and repair becomes more constant. The curve levels out, indicating that the software is more reliable and that further improvements become incremental.

**Hardware Reliability Curve:**

![[Pasted image 20231021111805.png]]
The hardware reliability curve, like the software reliability curve, represents the relationship between hardware failures and time. It typically exhibits a pattern where failures are more likely to occur at the beginning and end of a hardware component's life cycle:

1. **Infant Mortality Phase:** Early in the hardware's life, there may be a higher rate of failures. This is often due to manufacturing defects or weaknesses that become apparent shortly after the hardware is put into use.

2. **Random Failures Phase:** After the initial phase, hardware failures are relatively random and unpredictable, following a relatively constant failure rate.

3. **Wear-Out Phase:** Towards the end of its life cycle, the hardware component may experience an increase in failures due to aging, wear, and tear.

Both software and hardware reliability curves aim to understand and visualize how reliability changes over time, with software emphasizing the detection and correction of defects and hardware emphasizing the physical wear and tear that can lead to failures. Effective reliability engineering and testing aim to improve reliability and extend the duration of the steady-state phase while minimizing infant mortality and wear-out phases.
### McCall Quality Model
The McCall Quality Model is a well-known framework for understanding and assessing software quality. It was developed by John McCall and his colleagues in the 1970s and is based on the idea that software quality can be broken down into multiple, interrelated factors. These factors are organized into three categories: product revision, product transition, and product operation.

Here are some key notes about the McCall Quality Model:

**1. Product Revision (IPT)**:
   - **Correctness:** This factor assesses whether the software performs its intended functions accurately and without errors.
   - **Reliability:** Reliability evaluates the software's ability to maintain its performance over time and under various conditions.
   - **Efficiency:** Efficiency relates to the software's ability to execute tasks in a resource-efficient manner, such as using minimal system resources.
   - **Integrity:** Integrity measures the security and protection of data and functions from unauthorized access or tampering.
   - **Usability:** Usability assesses how user-friendly the software is and how easy it is for users to interact with it.
   - **Maintainability:** Maintainability measures how easily the software can be modified or updated, including aspects like code readability and modifiability.

**2. Product Transition (CCPT)**:
   - **Portability:** Portability assesses how easily the software can be moved to different environments or platforms.
   - **Reusability:** Reusability evaluates how well the software components can be reused in other applications, potentially reducing development time and cost.

**3. Product Operation (CEVT)**:
   - **Correctness:** Correctness also applies in the product operation phase, as it's important that the software continues to operate correctly during actual use.
   - **Efficiency:** Efficiency in product operation ensures that the software performs its tasks quickly and with minimal resource usage, even in a production environment.
   - **Security:** Security measures are critical during product operation to protect the software from threats and vulnerabilities.
   - **Integrity:** Ensuring the integrity of data and functions remains important during the operational phase.
   - **Usability:** Usability is crucial during product operation to ensure that users can effectively and efficiently interact with the software.
   - **Accuracy:** Accuracy is about the software continuing to produce accurate results during operation.

These notes provide an overview of the McCall Quality Model and the various factors that contribute to software quality at different stages of its lifecycle. The model has been influential in the field of software engineering and quality assurance, providing a structured way to think about and assess software quality. It's worth noting that while the McCall Quality Model has been historically significant, it has been further refined and expanded upon by subsequent models and standards.

### CMM
CMM in software engineering stands for "Capability Maturity Model." The Capability Maturity Model is a framework used to assess and improve an organization's software development and management processes. The primary goal of the CMM is to help organizations enhance their software development and maintenance processes, resulting in higher-quality software products.

There have been multiple versions of the Capability Maturity Model over the years, with the most well-known one being the CMM for Software (CMM-SW), also known as the Software CMM. The Software CMM was initially developed by the Software Engineering Institute (SEI) at Carnegie Mellon University in the United States and has been widely adopted as a benchmark for assessing an organization's software development practices.

The CMM for Software consists of five maturity levels, which represent increasing levels of process maturity and capability:

1. **Initial (Level 1):** At this level, processes are often ad hoc, chaotic, and unpredictable. There is a lack of control and documentation of software development processes.

2. **Repeatable (Level 2):** Organizations at this level have established basic project management and process controls, which enable them to repeat successful practices. Key processes are documented and followed.

3. **Defined (Level 3):** At this level, organizations have well-defined and standardized software development processes. There is a focus on process improvement, and organizations collect data to analyze and improve processes continuously.

4. **Managed (Level 4):** Organizations at this level use quantitative data to manage and control processes. The focus is on optimizing processes and achieving higher efficiency and quality.

5. **Optimizing (Level 5):** At the highest level, organizations continuously improve their processes to achieve excellence. They use innovative and advanced techniques to further enhance their processes.

Organizations can use the CMM as a roadmap to improve their software development practices. By assessing their current maturity level and identifying areas for improvement, they can gradually advance through the maturity levels to achieve higher levels of process capability and product quality.

It's important to note that the Capability Maturity Model has evolved over time, and its successor, the Capability Maturity Model Integration (CMMI), has become the more widely recognized framework for process improvement and assessment in various domains, including software engineering. CMMI is more comprehensive and extends beyond just software development to cover other aspects of an organization's processes.

## Reliability metric of software products 
Reliability is a crucial quality attribute for software products, as it measures the software's ability to perform its functions consistently and correctly over time. There are several metrics and methods to assess the reliability of software products:

1. **Failure Rate:** This metric measures the frequency of software failures over a specific period. It's often expressed as the number of failures per unit of time.

2. **Mean Time Between Failures (MTBF):** MTBF is the average time between two consecutive failures. It provides an estimate of how long the software can run without experiencing a failure. A higher MTBF indicates better reliability.

3. **Mean Time to Failure (MTTF):** MTTF represents the average time it takes for a software component to fail. Like MTBF, a higher MTTF indicates better reliability.

4. **Availability:** Availability measures the percentage of time that the software is operational and available for use. It's the complement of downtime, and higher availability is associated with higher reliability.

5. **Fault Density:** This metric quantifies the number of defects or faults in a software product relative to its size (e.g., lines of code or function points). Lower fault density is typically indicative of better reliability.

6. **Failure Density:** Similar to fault density, failure density measures the number of failures or defects in the software product relative to its size. It provides insights into the reliability of specific software components.

7. **Reliability Block Diagrams (RBDs):** RBDs are graphical representations of a system's reliability, showing how individual components or subsystems contribute to the overall reliability. They can help identify critical points of failure.

8. **Failure Mode and Effects Analysis (FMEA):** FMEA is a structured approach to identifying and prioritizing potential failure modes in a system, assessing their effects, and planning for mitigation. It's commonly used in critical systems to improve reliability.

9. **Mean Time to Repair (MTTR):** MTTR measures the average time it takes to restore the software to working condition after a failure. A lower MTTR can contribute to higher overall reliability.

10. **Software Reliability Growth Models:** These models predict the improvement in software reliability over time as defects are identified and fixed. One such model is the Jelinski-Moranda model.

11. **Error Rate:** The error rate metric focuses on the number of errors encountered during software operation. A lower error rate indicates better reliability.

12. **Service Level Agreements (SLAs):** SLAs specify the expected reliability and uptime of a software system as a contractual commitment to users or customers.

To effectively assess and improve the reliability of a software product, it's important to combine various metrics and methods, conduct thorough testing (including stress and reliability testing), and implement best practices for software development and maintenance, such as code reviews, version control, and robust error handling mechanisms. Reliability should be a fundamental goal throughout the software development lifecycle.

### Reliability Growth model 
A reliability growth model is a mathematical model of how software reliability
improves as errors are detected and repaired.

####  Jelinski and Moranda model
The simplest reliability growth model is a step function model where it is
assumed that the reliability increases by a constant increment each
time an error is detected and repaired.
![[Pasted image 20231021113500.png]]

#### Littlewood and Verallâ€™s model
This model allows for negative reliability growth to reflect the fact that
when a repair is carried out, it may introduce additional errors. It also
models the fact that as errors are repaired, the average improvement
to the product reliability per repair decreases

## Software Quality Management System
### Intro 
A quality management system (often referred to as quality system) is the principal methodology used by organisations to ensure that the
products they develop have the desired quality.

### Quality system activity
The quality system activities encompass the following:
- Auditing of projects to check if the processes are being followed.
- Collect process and product metrics and analyse them to check if
quality goals are being met.
- Review of the quality system to make it more effective.
- Development of standards, procedures, and guidelines.
- Produce reports for the top management summarising the
effectiveness of the quality system in the organisation.
### Evolution of Quality systems
Quality systems have rapidly evolved over the last six decades. Prior to World
War II, the usual method to produce quality products was to inspect the
finished products to eliminate defective products.
![[Pasted image 20231021113950.png]]

Quality Control (QC), Quality Assurance (QA), and Total Quality Management (TQM) are three related concepts in the field of quality management. They are often used in a progression to ensure product or service quality, and their evolution reflects an increasing emphasis on continuous improvement and customer satisfaction.

1. **Quality Control (QC):**
   Quality Control is a process-oriented approach focused on identifying and correcting defects in the final product or service. It involves inspecting and testing a representative sample of the output to ensure it meets predefined quality standards. If defects are found, corrective actions are taken. QC is a reactive approach, primarily concerned with maintaining consistency in the final product.

2. **Quality Assurance (QA):**
   Quality Assurance is a more proactive and comprehensive approach to quality management. QA encompasses the processes and systems used to create a product or service and ensures that they are designed, implemented, and maintained in a way that meets established quality standards. It involves preventing defects rather than just detecting and correcting them. QA aims to ensure that the entire process is capable of delivering a quality product consistently.

3. **Total Quality Management (TQM):**
   Total Quality Management is a holistic and organization-wide approach to quality. It involves a cultural shift in which every employee is responsible for maintaining and improving quality at all levels of the organization. TQM emphasizes customer satisfaction, continuous improvement, and the involvement of all employees in quality-related decisions and actions. TQM principles include customer focus, leadership, employee involvement, process improvement, and fact-based decision making.

**Evolution of Quality Systems:**
- **Inspection and QC (Historical Approach):** In the past, quality control was primarily based on the inspection of final products. This approach was reactive and focused on identifying defects after they occurred.

- **Introduction of QA (Mid-20th Century):** As industries expanded and products became more complex, a need arose for a more systematic approach to quality. This led to the development of Quality Assurance, which emphasized the importance of well-defined processes and procedures to ensure product quality.

- **TQM and Continuous Improvement (Late 20th Century):** TQM gained prominence in the late 20th century, driven by the works of quality gurus like Deming and Juran. TQM emphasized the involvement of all employees in quality improvement, customer focus, and a shift from mere compliance to continuous improvement. It represented a broader and more strategic perspective on quality.

- **Integration of Quality Approaches (Contemporary):** Modern quality management systems often integrate elements of QC, QA, and TQM. They recognize that all three approaches have their place in maintaining and improving quality. Companies implement robust QA systems and incorporate TQM principles to ensure ongoing quality while also using QC techniques when necessary to verify product quality.

In summary, the evolution of quality systems has moved from a reactive QC approach to a proactive QA approach, ultimately culminating in TQM, which focuses on continuous improvement and a culture of quality throughout an organization. In practice, a well-rounded quality management system may incorporate elements of all three approaches to achieve the best results and meet customer expectations.

## Software quality attribute
### What are these attribute
- Mention all ISO 9000 
### Functions of these 
- **Planning**: SQA involves creating a quality assurance plan that outlines the quality objectives, processes, and activities to ensure adherence to quality standards.
- **Process Management**: SQA defines and manages the development and testing processes to ensure that they follow best practices and standards.
- **Standards and Procedures**: It establishes and enforces coding standards, documentation practices, and testing procedures to maintain consistency and quality.
- **Reviews and Audits**: SQA conducts regular reviews and audits of software artifacts to identify defects, deviations from standards, and opportunities for improvement.
- **Testing and Validation**: SQA oversees the testing process to ensure that the software meets quality requirements, and it helps in the validation of the software against user needs.
- **Training and Skill Development**: It facilitates training and skill development programs for the development and testing teams to enhance their knowledge and skills related to quality assurance.
- **Documentation and Reporting**: SQA creates and maintains documentation related to quality assurance activities and provides reports on the quality status of the software.

## Reviews
In the context of software development, reviews refer to the systematic and structured examination of software artifacts, such as code, design documents, requirements, and other project-related documentation, to evaluate their quality, correctness, and adherence to established standards and best practices. Reviews play a crucial role in the software development process, and they are essential for several reasons:

1. **Error Detection**: Reviews help identify errors, defects, or issues early in the development process. Finding and addressing these issues at an early stage can significantly reduce the cost and effort required to fix them compared to discovering them in later phases of development or in the production environment.

2. **Quality Assurance**: Reviews contribute to the overall quality assurance process by ensuring that software artifacts meet established quality standards, coding guidelines, and best practices. This helps maintain a high level of quality and consistency in the software.

3. **Knowledge Sharing**: Reviews provide an opportunity for team members to share their knowledge and expertise. They allow team members to understand and learn from each other's work, leading to improved skills and better collaboration.

4. **Documentation and Traceability**: Reviews help ensure that documentation and requirements are accurately represented in the software. This is critical for maintaining traceability and ensuring that the software meets the intended purpose and requirements.

5. **Risk Management**: Identifying issues and potential risks during reviews allows teams to take corrective actions and mitigate risks before they impact the project's success. This proactive approach helps in delivering software on time and within budget.

6. **Continuous Improvement**: Through reviews, teams can gather feedback and insights, which can be used to improve development processes and practices. This fosters a culture of continuous improvement within the development team.

7. **Regulatory and Compliance Requirements**: In industries with regulatory or compliance requirements, reviews are often a mandatory part of the process to ensure that software products meet the necessary standards and regulations.
### Three ways to do it 
#### Informal meeting 
#### Formal presentation
#### Formal technical review 
- It is software quality assurance performed by software engineers 
- Identify error in logic , functionality and implementation
- Check if all the req are implemented / not
- also check if software is according to the standards
- It is 4 step process
Formal Technical Reviews (FTRs) are a structured approach to evaluating software artifacts in software development. The following are explanations of the four steps involved in FTR, which include review meetings, review reporting and record-keeping, review guidelines, and sample-driven reviews:

1. **Review Meeting**:

   - **Explanation**: The review meeting is a formal, structured discussion where a group of participants, including developers, testers, subject matter experts, and other stakeholders, come together to evaluate a specific software artifact. This step is central to the FTR process.

   - **Purpose**: The primary goal of the review meeting is to collaboratively assess the software artifact, identify defects, discrepancies, and potential improvements, and decide on actions to address the identified issues.

   - **Key Elements**:
     - **Opening**: The meeting begins with an introduction, led by a moderator or facilitator, where the purpose and agenda of the review are explained.
     - **Presentation (Optional)**: The author of the artifact may provide context or clarifications through a brief presentation.
     - **Discussion**: Reviewers discuss their findings, focusing on objective evaluation and constructive feedback.
     - **Issue Resolution**: Identified issues and defects are discussed, and resolutions are proposed.
     - **Documentation**: Meeting notes are taken, capturing discussions, issues, and resolutions.
     - **Closing**: The meeting concludes with a summary of discussions and agreed-upon actions, including a plan for follow-up activities, such as re-reviews.

2. **Review Reporting and Record Keeping**:

   - **Explanation**: After the review meeting, the findings, issues, and resolutions from the meeting are documented in a formal review report. This report serves as a record of the review process and its outcomes.

   - **Purpose**: The review report provides a structured and comprehensive summary of the review activities, findings, and decisions, and it serves as a reference for further actions and process improvement.

   - **Key Components**:
     - **Documenting Findings**: Findings from the meeting notes are documented, often categorized by severity or priority.
     - **Formalizing the Report**: The meeting notes are transformed into a formal review report, presenting the findings in a structured manner.
     - **Distribution**: The review report is distributed to relevant stakeholders, including the author, reviewers, and management.
     - **Reference and Follow-Up**: The report is used as a reference for future actions and to track progress on issue resolutions.
     - **Archiving**: The review report and associated documentation are archived for historical and audit purposes.

3. **Review Guidelines**:

   - **Explanation**: Review guidelines are established rules and principles that guide the FTR process. They define the expectations, responsibilities, and procedures for conducting a review.

   - **Purpose**: Review guidelines help ensure consistency and quality in the review process, providing a common framework for all participants to follow.

   - **Key Elements**:
     - **Roles and Responsibilities**: Guidelines define the roles of the participants, including the moderator, author, and reviewers.
     - **Review Criteria**: They specify the criteria that the reviewers should consider when evaluating the artifact.
     - **Meeting Structure**: Guidelines outline the structure of the review meeting, including the agenda and the order of activities.
     - **Documentation Standards**: Guidelines may specify how findings should be documented and reported.
     - **Tailoring**: Guidelines can be tailored to the specific needs and context of the project or organization.

4. **Sample-Driven Review**:

   - **Explanation**: Sample-driven reviews involve reviewing specific, representative sections or samples of a larger software artifact, such as source code or documentation, rather than examining the entire artifact.

   - **Purpose**: This approach is used to streamline the review process, especially when the full review of a large artifact is impractical or time-consuming. Sample-driven reviews aim to identify common issues and trends based on the selected samples.

   - **Key Elements**:
     - **Sample Selection**: Specific sections or samples are chosen for review. These samples should be representative of the overall artifact.
     - **Focused Evaluation**: Reviewers concentrate their efforts on analyzing the selected samples, looking for patterns and issues that may apply more broadly.
     - **Efficiency**: Sample-driven reviews save time and resources compared to reviewing the entire artifact while still providing valuable feedback.

The steps of FTR, including the review meeting, review reporting and record-keeping, review guidelines, and sample-driven reviews, help ensure that software artifacts are thoroughly evaluated, defects are identified, and corrective actions are taken, contributing to software quality and adherence to standards. 

Formal Technical Reviews (FTRs) offer several benefits in the context of software development and quality assurance. These benefits contribute to improved software quality, reduced defects, and a more efficient development process. Some of the key benefits of FTR include:

1. **Early Defect Detection**: FTRs enable the early detection and identification of defects, errors, and issues in software artifacts. By addressing these problems in the early stages of development, FTRs help prevent defects from propagating to later, more costly phases of the development process.
    
2. **Quality Improvement**: FTRs contribute to a higher level of software quality by ensuring that the software meets predefined standards, adheres to coding guidelines, and complies with requirements. This leads to more reliable and robust software products.
    
3. **Risk Mitigation**: FTRs help in identifying potential risks and issues, allowing teams to take corrective actions before problems become critical. This proactive approach reduces the likelihood of project delays and budget overruns.
    
4. **Efficiency and Cost Savings**: By addressing issues early in the development process, FTRs save time and resources. It is more cost-effective to fix defects during the design or coding phases than after deployment.
    
5. **Knowledge Sharing and Learning**: FTRs provide a platform for team members to share their knowledge, expertise, and best practices. Team members learn from each other's work, leading to improved skills and better collaboration.
    
6. **Process Improvement**: Insights and feedback gathered during FTRs can be used to enhance development processes and practices. This fosters a culture of continuous improvement within the development team.
## Improving reliability 
Certainly, here are six key approaches to improve the reliability of software:

1. **Thorough Testing**: Conduct comprehensive functional, non-functional, and regression testing to identify and address issues and ensure the software meets its requirements.

2. **Code Reviews**: Regularly perform peer code reviews to identify coding errors, potential defects, and maintain coding standards.

3. **Static Analysis Tools**: Use automated static analysis tools to scan source code for vulnerabilities and coding errors.

4. **Robust Error Handling**: Implement robust error and exception handling to prevent crashes and gracefully recover from errors.

5. **Configuration Management**: Use configuration management practices to control and track changes, ensuring consistency.

6. **Continuous Improvement**: Implement quality assurance processes and continuously improve development processes to prevent defects and enhance reliability.

