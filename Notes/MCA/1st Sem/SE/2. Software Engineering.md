## Data modeling 
Data modeling is the process of creating a conceptual representation of data and its relationships in order to understand and manage data effectively. It is a crucial step in database design and information system development. Data modeling helps in organizing and structuring data so that it can be stored, accessed, and analyzed efficiently

if application is data intensive then ER model can be used to produce conceptual data model of system

ER diagram servers as a documentation tool
### Obj of data modelling 
- to identify the data items in application and relationship b/w those identified data items 
- to create conceptual schema for the data.

#### ER Model
- IEEE defines "diagram that depicts the set of real world entities and logical relationship among them"
- Basic units 
	- Entities
	- type of entities
	- relationship 
	- Attributes
**Already covered in DBMS**

## Structural analysis 
### What is it 
- top-down approach in which focus is on refining the problem with the help of function performed in the problem domain and data produced by system.
### Principles of SA
- Top-down decomposition approach
- application of divide and conquer principle :- each high level function is independently decomposed into detailed function

### Objective of SA
- To provide graphical representation to develop new software or to enhance the existing software

### Tools for structural analysis
#### Data flow diagram
##### Intro
- diagram that depicts data sources , data sinks , data storage , processes performed on data as nodes and logical flow of data as the links b/w the nodes
- it is also called Data flow graph / Bubble Chart/ Bubble Graph
##### DFD vs Flow Chart 
- DFD should not be confused with flow chart . DFD represent the flow of data whereas flow chart represent flow of control/logic 

##### Obj of DFD
- Represent the system data in hierarchical order with req lvl of details
- Depicts the processes according to the defined user req and scope of software 
- To provide the overview of transformation that occur in the input data within the system to produce the output
- DFD depicts the flow of data within the system and considers a system that transform the input into req outputs 
- WHen the system is complex then data needs to be transformed by various steps to produce the output . These steps are required to refine the requirement 
##### +ve of DFD
- Provides the functional decomposition of proposed system
- provides the details of data flow in proposed system
- communicates the existing system knowledge to stakeholder
##### Units of DFD
- DFD shows the flow of data through a system . it views the system as function that transforms the input into desired outputs . any complex system does not perform the transformation in the single step but undergoes into number of transformation before it becomes the output
- Process:- 
	- The agent that performs the transformation from one state to another is called a process or bubble . Process is represented by named/ labeled circle.
- Data flow:-
	- Data flow is represented by named / labeled arrow
- Data store:- 
	- Data store is represented by open rectangle . Open rectangle should be named or labeled 
- Source or sink / External entity:- Source (originator of data) and sink(consumer of data) is represented by labeled rectangle 
- ![[Pasted image 20231008151125.png]]

##### Guidelines for creating DFD
- Meaninful names to be given to DFD notation . eg . in general the verbs should be used for naming a process where nouns should be used for the external entity , data flow and data store
- DFD should be created in organised order so that it can be easily understood
- Each process should be numbered uniquely 
- All the symbols used in DFD should be labeled

##### Synchronous and Asynchronous Operations of DFD
- ![[Pasted image 20231008151757.png]]

##### Developing DFD model of system
- DFD shows the flow of data through system . it views the system as function that transforms input into desired outputs
**LEVELED DATA FLOW DIAGRAMS**
- Any complex system does not perform the transformation in single step but undergoes into the number of transformation before it becomes the output
- Context diagram :- context diagram is most abstract data flow 
	- it is the most abstract data flow representation of system. it represent the entire system as a single bubble . In fig the banking system is represented as single process . Process banking system is represented by circle . User is the source and user is sink . Source and sink are represented by rectangle . Data flow represented by arrow
	- To develop the context diagram of system is to be analysed to identify the different type users who would be using system and the kinds of data they would be inputting to the system and the kinds of data they would be receiving from the system . Context lvl DFD is called as 0-lvl DFD
	- ![[Pasted image 20231008152638.png]]
	- 
- LVL 1- DFD :- Context lvl DFD is decomposed into LVL1 - DFD
	- User entity (source) is related to several processes in the bank which includes register. User support and provide cash . The transaction can be performed if the user is already registered in the bank
	- once user is registered . he can perform the transaction by the processes namely Deposit cheque , deposit cash and withdraw cash
	- The line in the process symbol indicates the level of process and contains a unique identifier in the form of number 
	- The user info such as name , address and account number is stored in data store User_detail
	- It is not necessary that the user is registered in the bank to have the demand draft then the details of the cash and date are stored in the data store , DD_detail
	- Once the demand draft is prepared then its receipt is provided to the user (sink)
	- ![[Pasted image 20231008153505.png]]
- LVL2 - DFD :- Processes of LVL 1 is decomposed into LVL 2
	- ![[Pasted image 20231008153536.png]]
	- 
- LVL 3 - DFD :- Processs of LVL 2 is decomposed into LVL 3
	- ![[Pasted image 20231008153607.png]]
- this process of decomposition is up to N level and it depends upon the details required in DFD

##### Informal verfication of DFD and data dictionary 
- After construction of DFD , an analyst can perform the walkthrough and look for error 
	- Unlabelled data flows 
	- Missing data flow 
	- Missing processes 
	- Consistency not maintained during the refinement
### Steps of SA
- study the physical environment of system
	- During this step DFD of current non automated system is drawn which shows the input and output data flows 
	- How the data flows through the system ?
	- what are the different processs operating on data 
	- While drawing the DFD for physical env the analyst have to interact with the users to determine the overall process from the point of view of data 
	- In this step context diagram is created in which the entire system is treated as a single process . in the context diagram all the inputs to the process , outputs from process , sources and sinks are identified and drawn
- Decomposition of process
	- Based on top down refiendment the context lvl process is decomposed into diff levels
- Draw logical equivalents of DFD for physical system
	- During this step the DFD physical env is taken and all the specific data flows are represented by their logical equivalents . the physical bubbles may be replaced by the logical processes . bubbles that do not transforms data in any form may be deleted from DFD
- Modeling the current system:- During the first two steps the current system is modeled and the next step is to model the new system after the changes have been incorporated . what are data flows and what are the major processes in this new system must be determined by analyst based upon his experience and vision
- Establishing the man machine boundary:- The next step is to establish the man machine boundary by specifying that what needs to automated and what will remain manual in new system.

## Data dictionary 
### Intro
- When system analysis is performed then it is essential for the system analyst to decide the necessary data for designing the application 
- DFD gives the info about the data flows and data stores of system so the individual data item of data flows and data stores can be catalogued . The catalogue can bring out that if any data is missing or duplicated . Such a catalogue is called data dictionary or data repository
- DD is essential tool in SDP to describe the DD sufficient info should be collected about the data items
- DD is a data repository which stores the info about system data at one place
- The DD is a reference work of data about data (metadata)
### Format of DD
![[Pasted image 20231008160749.png]]
![[Pasted image 20231008160848.png]]
![[Pasted image 20231008160906.png]]
### Others 
- DD should contains info about the following 
	- Data flows
		- ![[Pasted image 20231008161205.png]]
		- 
		- Is described by means of DFD . these are path along which data travels and data stores and places where data is stored. it can be said that data flow are data structure in motion and data stores are data structure in rest
	- Data stores
		- ![[Pasted image 20231008161225.png]]
	- Processes
		- ![[Pasted image 20231008161249.png]]
	- External Entities
		- ![[Pasted image 20231008161306.png]]
### Data Dictionary classes
- Data element :- it is the smallest unit of data and is also called as data item. each data element should be properly documented in data dictionary
- Data structure:- Once data items are defined then DS req by particular system under consideration can be identified . Data elements are combined to form DS
- ![[Pasted image 20231008161524.png]]
### use of DD
- important documentation which would be useful to maintain the system
- purpose of DD is to provide a source of reference for analyst , user and designer
- Analyst use the DD for collecting , documenting and for organising the specific facts about system
- It eliminates the redundancy 
- helps in validation of DFD
- becomes the starting point for developing the screen reports and forms

## Structure chart 
### What is it 
- Graphic depiction of decomposition of problem . Structure chart illustrate the partitioning of modules into sub modules . SC are used to show the communication b/w modules
- show the control and dataflow b/w modules
- SC represent the hierarchical structure of modul
- helps the programmer to divide the problems into smaller parts so that problem can be easily understood 
- it is used as tool to help in software design
- in design stage the structure chart is drawn and is used by clients and by designer to communicate b.w them
### Components of SC
- Modules 
- Connection b/w modules
- Communication b/w modules
### Basic building block of SC
![[Pasted image 20231008162330.png]]
### Modules
- ![[Pasted image 20231008162541.png]]
- types
	- Control module: Module at top in figure is called as root module also called control module
	- Subordinate module:- Modules can be subordinate module . In fig module i.e m1 , m2 are subordinate modules and these are also represented by rectangles
	- Library modules:- usually represented by rectangle with double edge . comprises the frequently called modules. when the module is invoked by many other module then it is made into library module 
	- ![[Pasted image 20231008162815.png]]
### Arrow 
![[Pasted image 20231008162912.png]]
### Data couple
![[Pasted image 20231008162946.png]]

### Control couple
![[Pasted image 20231008163022.png]]

### Diamond box
- it is used for indicating decision and depending upon decision one of the sub modules can be invoked
- In figure it is shown that module Print valid customer No . takes the decision based on the flag (control couple) valid customer no . whether to print a customer no. or to print an error message . Based on the decision the relevant module can be invoked ie. if customer no is valid then the print customer no . module is invoked otherwise module print error message is invoked
- ![[Pasted image 20231014153943.png]]

### Arc with arrow head 
- represent iteration
- a loop around control flow arrows denotes that the respective modules are invoked repeatedly 
- The number of modules that come within the loop is shown by number of arrows invoking modules
- In fig it is shown that three sub modules . Get customer no, validate customer no and print customer no are invoked repeatedly by the print valid customer no.
- ![[Pasted image 20231014154157.png]]

### Layered approach
- In any structure chart there should be one and only one module at the top called root
- There should be at most one control relationship b/w any two modules in the structure chart i.e if a module A is invokes module B then module B cannot invoke module A
- The layered design and poorly layered design is shown in fig
- ![[Pasted image 20231014154407.png]]

### Flow Chart vs Structure chart
- flow chart represent the flow of control/logic but it is difficult to identify the different modules of program from its flow . Structure chart identify the different modules of software
- The flow charts do not show the data exchange b/w different module but structure chart represent data exchange b/w modules
### Steps to create structure chart
1. First depict relation of modules from top to bottom
2. Conceptualise the main sub tasks that must be performed by program to solve the problem
3. Focus is given on each subtask and conceptualised that how each sub task can be broken into smaller tasks
4. Once the Strucutred chart is designed then the bottom up implementation of each module 
![[Pasted image 20231014154842.png]]

![[Pasted image 20231014154913.png]]
![[Pasted image 20231014154939.png]]
![[Pasted image 20231014155024.png]]

## Object oriented methodologies
### What is modeling 
- It is an abstraction of something for the purpose of understanding it before building it 
- Because real system that we want to study are generally very complex . In order to understand the real system , we have to simplify the system
- So a model is an abstraction that hides the non-essential characteristics of a system and highlights those features, wihch are pertinent to understand it 
- Most of modelling techq , for the analysis and design used graphics so the symbols are used acc to certain set of rules of methodology for communicating the complex relationship of information more clearly than the descriptive text
- need
	- To test physical entity before actual building it 
	- To set the basics for communication b/w the client and developer 
	- For finding the different alternatives
	- For reducing the complexity in order to understand the system
### Object oriented methodologies
![[Pasted image 20231014162738.png]]
#### Steps of OOP
![[Pasted image 20231014162806.png]]

### Structure vs OOP
![[Pasted image 20231014162846.png]]

### +ves of OOM
![[Pasted image 20231014162914.png]]

### Type of OOM
![[Pasted image 20231014162942.png]]
![[Pasted image 20231014163008.png]]
![[Pasted image 20231014163029.png]]
![[Pasted image 20231014163057.png]]
![[Pasted image 20231014163120.png]]

### What is UML
![[Pasted image 20231014163153.png]]


### Object Modelling Techniques
![[Pasted image 20231014163257.png]]
![[Pasted image 20231014163325.png]]
![[Pasted image 20231014163346.png]]
![[Pasted image 20231014163411.png]]
![[Pasted image 20231014163433.png]]
![[Pasted image 20231014163504.png]]
![[Pasted image 20231014163522.png]]
![[Pasted image 20231014163544.png]]
![[Pasted image 20231014163600.png]]
![[Pasted image 20231014163622.png]]
![[Pasted image 20231014163720.png]]
![[Pasted image 20231014163744.png]]
![[Pasted image 20231014163802.png]]
![[Pasted image 20231014163825.png]]
![[Pasted image 20231014163855.png]]
![[Pasted image 20231014163926.png]]
![[Pasted image 20231014163944.png]]
![[Pasted image 20231014164001.png]]

## Coding Standards 
### ISO 9126
ISO 9126 is indeed a recognized international standard. ISO/IEC 9126, titled "Software engineering - Product quality," is a standard that defines a framework for the evaluation of software product quality. It provides a set of quality characteristics and sub-characteristics that can be used to assess the quality of software. The standard is often used by software developers, quality assurance professionals, and other stakeholders to evaluate and improve the quality of software products.

1. **Functionality:**
   - **Suitability:** This sub-characteristic assesses the software's capability to meet specific needs and requirements. It considers how well the software satisfies the intended functions and features.
   - **Accuracy:** Accuracy evaluates the precision and correctness of the software's output. It's about ensuring that the software produces accurate results.
   - **Security:** Security measures the software's ability to protect data and functionalities from unauthorized access, ensuring that it is robust against security threats.
   - **Interoperability:** Interoperability checks whether the software can interact and work effectively with other systems, ensuring compatibility and seamless communication.

2. **Reliability:**
   - **Fault Tolerance:** Fault tolerance evaluates the software's ability to continue functioning correctly even in the presence of faults or errors. It measures the system's resilience to failures.
   - **Maturity:** Maturity assesses how well the software has evolved and improved over time. It implies that a mature software is more reliable and stable.
   - **Recoverability:** Recoverability refers to the software's ability to recover from failures or errors, ensuring that it can return to a working state after encountering issues.

3. **Usability:**
   - Usability examines how user-friendly the software is. It focuses on factors like ease of use, intuitiveness, and overall user satisfaction.

4. **Efficiency:**
   - **Time Behavior:** Time behavior assesses how quickly the software performs its tasks, including response times and processing speed.
   - **Resource Behavior:** Resource behavior evaluates how efficiently the software utilizes system resources, such as CPU, memory, and network bandwidth. Efficient software should use minimal resources.

5. **Maintainability:**
   - **Testability:** Testability measures how easily the software can be tested. It includes aspects like whether it's straightforward to create and run tests to verify its functionality and quality.
   - **Changeability:** Changeability assesses how easily the software can be modified or updated. It includes factors like code maintainability and how well it accommodates changes without introducing new errors.
   - **Analyzability:** Analyzability is about how easy it is to understand and analyze the software's structure and code to identify and diagnose issues.
   - **Stability:** Stability evaluates the software's ability to maintain its quality and performance even after modifications or updates. It should not become less reliable or efficient when changes are made.

6. **Portability:**
   - **Installability:** Installability measures how easily the software can be installed and set up on different platforms and environments.
   - **Adaptability:** Adaptability assesses how well the software can adapt to different operating environments and configurations.
   - **Replaceability:** Replaceability evaluates how easily the software can be replaced by another system or component without causing disruption to the overall system.

These quality characteristics and sub-characteristics provide a structured way to assess and evaluate software quality, ensuring that software meets user expectations, performs reliably, and can be maintained and adapted effectively.
ISO/IEC 9126 helps organizations and software developers to set quality objectives, make informed decisions about software development and maintenance, and communicate quality requirements to all stakeholders.
### Software vs Hardware relability 
**Software Reliability:**

Software reliability refers to the probability that a software system will perform its intended functions without failures over a specified period of time and under certain conditions. It is a measure of the software's ability to operate correctly and consistently. Software reliability is related to how well the software behaves under various circumstances, and it involves factors like error rates, availability, and mean time between failures (MTBF).

**Hardware Reliability:**

Hardware reliability, on the other hand, focuses on the reliability of physical components and devices within a computer system. It assesses the probability that hardware components (e.g., processors, memory, storage devices) will operate without failures over a specific duration and under specific environmental conditions. Hardware reliability often involves metrics like mean time between failures (MTBF), mean time to repair (MTTR), and failure rates.

**Software Reliability Curve:**
![[Pasted image 20231021111836.png]]
The software reliability curve represents the relationship between the cumulative number of defects or failures and time. It typically starts high and decreases as the software is tested and used. The shape of the curve can vary depending on the software and its development and testing process. There are two key phases in the software reliability curve:

1. **Infant Mortality Phase:** In this phase, which occurs early in the software's life cycle, the number of defects or failures is relatively high. This is because initial testing and usage reveal many issues, and they are fixed as they are discovered. As a result, the number of defects decreases rapidly during this phase.

2. **Steady-State Phase:** In the steady-state phase, the software's reliability stabilizes, and the rate of defect discovery and repair becomes more constant. The curve levels out, indicating that the software is more reliable and that further improvements become incremental.

**Hardware Reliability Curve:**

![[Pasted image 20231021111805.png]]
The hardware reliability curve, like the software reliability curve, represents the relationship between hardware failures and time. It typically exhibits a pattern where failures are more likely to occur at the beginning and end of a hardware component's life cycle:

1. **Infant Mortality Phase:** Early in the hardware's life, there may be a higher rate of failures. This is often due to manufacturing defects or weaknesses that become apparent shortly after the hardware is put into use.

2. **Random Failures Phase:** After the initial phase, hardware failures are relatively random and unpredictable, following a relatively constant failure rate.

3. **Wear-Out Phase:** Towards the end of its life cycle, the hardware component may experience an increase in failures due to aging, wear, and tear.

Both software and hardware reliability curves aim to understand and visualize how reliability changes over time, with software emphasizing the detection and correction of defects and hardware emphasizing the physical wear and tear that can lead to failures. Effective reliability engineering and testing aim to improve reliability and extend the duration of the steady-state phase while minimizing infant mortality and wear-out phases.
### McCall Quality Model
The McCall Quality Model is a well-known framework for understanding and assessing software quality. It was developed by John McCall and his colleagues in the 1970s and is based on the idea that software quality can be broken down into multiple, interrelated factors. These factors are organized into three categories: product revision, product transition, and product operation.

Here are some key notes about the McCall Quality Model:

**1. Product Revision (IPT)**:
   - **Correctness:** This factor assesses whether the software performs its intended functions accurately and without errors.
   - **Reliability:** Reliability evaluates the software's ability to maintain its performance over time and under various conditions.
   - **Efficiency:** Efficiency relates to the software's ability to execute tasks in a resource-efficient manner, such as using minimal system resources.
   - **Integrity:** Integrity measures the security and protection of data and functions from unauthorized access or tampering.
   - **Usability:** Usability assesses how user-friendly the software is and how easy it is for users to interact with it.
   - **Maintainability:** Maintainability measures how easily the software can be modified or updated, including aspects like code readability and modifiability.

**2. Product Transition (CCPT)**:
   - **Portability:** Portability assesses how easily the software can be moved to different environments or platforms.
   - **Reusability:** Reusability evaluates how well the software components can be reused in other applications, potentially reducing development time and cost.

**3. Product Operation (CEVT)**:
   - **Correctness:** Correctness also applies in the product operation phase, as it's important that the software continues to operate correctly during actual use.
   - **Efficiency:** Efficiency in product operation ensures that the software performs its tasks quickly and with minimal resource usage, even in a production environment.
   - **Security:** Security measures are critical during product operation to protect the software from threats and vulnerabilities.
   - **Integrity:** Ensuring the integrity of data and functions remains important during the operational phase.
   - **Usability:** Usability is crucial during product operation to ensure that users can effectively and efficiently interact with the software.
   - **Accuracy:** Accuracy is about the software continuing to produce accurate results during operation.

These notes provide an overview of the McCall Quality Model and the various factors that contribute to software quality at different stages of its lifecycle. The model has been influential in the field of software engineering and quality assurance, providing a structured way to think about and assess software quality. It's worth noting that while the McCall Quality Model has been historically significant, it has been further refined and expanded upon by subsequent models and standards.

### CMM
CMM in software engineering stands for "Capability Maturity Model." The Capability Maturity Model is a framework used to assess and improve an organization's software development and management processes. The primary goal of the CMM is to help organizations enhance their software development and maintenance processes, resulting in higher-quality software products.

There have been multiple versions of the Capability Maturity Model over the years, with the most well-known one being the CMM for Software (CMM-SW), also known as the Software CMM. The Software CMM was initially developed by the Software Engineering Institute (SEI) at Carnegie Mellon University in the United States and has been widely adopted as a benchmark for assessing an organization's software development practices.

The CMM for Software consists of five maturity levels, which represent increasing levels of process maturity and capability:

1. **Initial (Level 1):** At this level, processes are often ad hoc, chaotic, and unpredictable. There is a lack of control and documentation of software development processes.

2. **Repeatable (Level 2):** Organizations at this level have established basic project management and process controls, which enable them to repeat successful practices. Key processes are documented and followed.

3. **Defined (Level 3):** At this level, organizations have well-defined and standardized software development processes. There is a focus on process improvement, and organizations collect data to analyze and improve processes continuously.

4. **Managed (Level 4):** Organizations at this level use quantitative data to manage and control processes. The focus is on optimizing processes and achieving higher efficiency and quality.

5. **Optimizing (Level 5):** At the highest level, organizations continuously improve their processes to achieve excellence. They use innovative and advanced techniques to further enhance their processes.

Organizations can use the CMM as a roadmap to improve their software development practices. By assessing their current maturity level and identifying areas for improvement, they can gradually advance through the maturity levels to achieve higher levels of process capability and product quality.

It's important to note that the Capability Maturity Model has evolved over time, and its successor, the Capability Maturity Model Integration (CMMI), has become the more widely recognized framework for process improvement and assessment in various domains, including software engineering. CMMI is more comprehensive and extends beyond just software development to cover other aspects of an organization's processes.

