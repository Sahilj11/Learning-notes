# UNIT 1
## intro

### 📘 **Definition of Data Mining**

- **Data Mining** is the process of extracting useful **knowledge or insights** from large volumes of data using **statistical and computational techniques**.

### 📦 **Types of Data Handled**

- Data can be:
    
    - **Structured** (e.g., relational databases),
        
    - **Semi-Structured** (e.g., XML, JSON),
        
    - **Unstructured** (e.g., text, images, videos).
        
- Stored in forms like:
    
    - **Databases**
        
    - **Data Warehouses**
        
    - **Data Lakes**
        

---

### 🎯 **Goal of Data Mining**

- To **discover hidden patterns, relationships, or trends** in the data.
    
- Helps in **making informed decisions** or **predicting outcomes**.



## 📘 **Introduction to Data Mining**

**Data Mining** is the process of discovering **interesting patterns, knowledge, trends, or relationships** from large amounts of data. It lies at the intersection of statistics, machine learning, artificial intelligence, and database systems.

---

## 📦 **1. Kind of Data to be Mined**

Data mining is not limited to one type of data. It can be applied to various types of data sources:

### a. **Relational Data**

- Structured data stored in tables (e.g., SQL databases).
    
- Example: Employee records, bank transactions.
    

### b. **Transactional Data**

- Sequence of transactions (e.g., sales, purchases).
    
- Example: Market basket data.
    

### c. **Spatial and Temporal Data**

- Data with location or time component.
    
- Example: Weather patterns, traffic analysis.
    

### d. **Text Data**

- Unstructured documents, emails, tweets.
    
- Example: Sentiment analysis, spam detection.
    

### e. **Web Data**

- Web pages, clickstreams, server logs.
    
- Example: User behavior tracking.
    

### f. **Multimedia Data**

- Images, audio, video.
    
- Example: Face recognition, video surveillance.
    

### g. **Sensor Data / IoT**

- Generated by sensors, smart devices.
    
- Example: Smart home devices, health monitors.
    

---
![](../../statics/Pasted%20image%2020250511135301.png)

## ⚙️ **2. Data Mining Functionalities


### 🔹 **A. Descriptive Mining (Understand the data)**

This category of data mining is concerned with finding patterns and relationships in the data that can provide insight into the underlying structure of the data. Descriptive data mining is often used to summarize or explore the data, and it can be used to answer questions such as: What are the most common patterns or relationships in the data? Are there any clusters or groups of data points that share common characteristics? What are the outliers in the data, and what do they represent?
1. **Clustering** – Grouping similar data items.
    - Example: Grouping customers by buying behavior.
2. **Summarization** – Getting an overview (mean, median, charts).
    - Example: Sales summary per region.
3. **Association Rule Mining** – Discovering item relationships.
    - Example: “If bread → buy butter” (Market Basket Analysis).
4. **Sequential Pattern Mining** – Discovering sequences.
    - Example: In a store: “Buy mobile → then mobile cover”.

### 🔹 **B. Predictive Mining (Predict unknown values)**

This category of data mining is concerned with developing models that can predict future behavior or outcomes based on historical data. Predictive data mining is often used for classification or regression tasks, and it can be used to answer questions such as: What is the likelihood that a customer will churn?

1. **Classification** – Assigning data to predefined classes.
    
    - Example: Classifying emails as spam or not spam.
        
2. **Regression** – Predicting numeric values.
    
    - Example: Predicting housing prices.
        
3. **Anomaly Detection (Outlier Detection)** – Identifying rare patterns.
    
    - Example: Detecting credit card fraud.
        

---

## 🛠️ **3. Technologies Used in Data Mining**

Data mining is enabled by a combination of several technologies:

### 🔧 a. **Database Systems**

- Efficient storage, indexing, and retrieval of data.
    

### 🤖 b. **Machine Learning**

- Algorithms for classification, clustering, etc.
    

### 📊 c. **Statistics**

- Fundamental for pattern recognition and model evaluation.
    

### 🧠 d. **Artificial Intelligence**

- Provides smart decision-making and learning capabilities.
    

### 🧱 e. **Data Warehousing**

- Integration and consolidation of large datasets.
    

### 🌐 f. **Information Retrieval**

- Fetching relevant data from unstructured sources like text.
    

### ⚡ g. **High Performance Computing**

- Required for mining massive datasets (Big Data).
    

---

## 🚀 **4. Applications of Data Mining**

### a. **Business and Marketing**

- Customer segmentation, churn prediction, targeted marketing.
    

### b. **Finance**

- Risk management, fraud detection, credit scoring.
    

### c. **Healthcare**
- Disease diagnosis, drug discovery, patient monitoring.

### d. **Retail**
- Market basket analysis, inventory prediction, recommendation engines.
    

### e. **Telecommunications**
- Network fault detection, customer behavior analysis.
    

### f. **Education**
- Student performance tracking, dropout prediction.
    

### g. **Web and Social Media**

- Clickstream analysis, sentiment analysis, trend detection.
    

---

## ⚠️ **5. Major Issues in Data Mining**

### 🔸 a. **Data Quality**

- Real-world data is often incomplete, noisy, or inconsistent.
    

### 🔸 b. **Scalability**

- Mining must handle terabytes/petabytes of data efficiently.
    

### 🔸 c. **Data Integration**

- Combining data from different sources is complex.
    

### 🔸 d. **Privacy and Security**

- Personal data must be protected from misuse.
    

### 🔸 e. **Algorithm Complexity**

- Algorithms must balance accuracy, speed, and memory use.
    

### 🔸 f. **Interpretability of Results**

- Results should be understandable and explainable to end users.
    

### 🔸 g. **Dynamic Data**

- Real-time or streaming data poses additional challenges.

## 📝 Summary Table

|Topic|Description|
|---|---|
|**Data Types**|Structured, semi-structured, unstructured, multimedia, web|
|**Functionalities**|Descriptive (clustering, association), Predictive (classification, regression)|
|**Technologies Used**|DBMS, ML, AI, Stats, Data Warehousing, HPC|
|**Applications**|Business, Finance, Health, Telecom, Retail, Education|
|**Major Issues**|Data quality, scalability, privacy, algorithm efficiency|


## 📘 What is **Data Preprocessing**?

**Data Preprocessing** is the process of transforming raw data into a clean and organized format so that it can be efficiently and accurately used for data mining and machine learning tasks.

It improves data quality by handling missing, noisy, inconsistent, or irrelevant data.

## ✅ Need for Data Preprocessing

Real-world data is often:
- **Incomplete** – Missing values or attributes
- **Noisy** – Contains errors, outliers, or random variations
- **Inconsistent** – Conflicts or duplications in data
- **Unstructured / Heterogeneous** – Different formats or representations

### ✴️ Why It's Important:
- Ensures **accuracy** of data mining results    
- Improves **algorithm performance**
- Reduces **processing time**
- Makes the data **understandable** and **analyzable**

## 🔁 Steps in Data Preprocessing

|Step|Description|
|---|---|
|**1. Data Cleaning**|Handle missing values, remove noise and duplicates|
|**2. Data Integration**|Combine data from multiple sources|
|**3. Data Reduction**|Reduce data size via sampling, aggregation, or dimensionality reduction|
|**4. Data Transformation**|Normalize, standardize, smooth, or construct new attributes|
|**5. Data Discretization**|Convert continuous data into categorical form|


### 💡 Example:

Suppose a dataset contains customer information with missing ages, inconsistent formats (e.g., "M"/"Male"), and income values in different currencies. Preprocessing would:

- Fill missing ages (e.g., with average)
    
- Standardize gender labels
    
- Convert income to a common currency
    
- Normalize age/income for modeling


## 📘 **Data Objects and Attribute Types**

### 🔹 What is a **Data Object**?

A **data object** (also called a record, instance, or sample) represents a single entity or item in the dataset about which information is collected.

🧾 **Examples**:

- A customer in a bank
    
- A student in a college database
    
- A product in an inventory system
    

Each data object is described using a **set of attributes (features)**.

## 🔹 **Attribute (Feature/Variable)**

An **attribute** is a property or characteristic of a data object.  
Each attribute has a **type**, and the type determines the mathematical operations that can be applied to it.

## 📊 **Types of Attributes**

There are **four main types** of attributes in data mining:

|Attribute Type|Description|Examples|Allowed Operations|
|---|---|---|---|
|**Nominal**|Labels or names; no natural order|Gender (Male/Female), Color (Red/Blue)|=, ≠|
|**Ordinal**|Values with an inherent order, but no fixed spacing|Rankings (1st, 2nd, 3rd), Education Level (High school < Bachelor < Master)|=, ≠, <, >|
|**Interval**|Numeric with meaningful differences, but **no true zero**|Temperature in Celsius, Dates|+, −|
|**Ratio**|Numeric with a true zero point; allows all arithmetic|Age, Salary, Distance, Height|+, −, ×, ÷|

---

## 🧪 Examples of Each Type

|Attribute|Type|Why|
|---|---|---|
|Blood Type|Nominal|Categories with no order (A, B, AB, O)|
|Movie Ratings|Ordinal|1 to 5 stars imply order|
|Date of Birth|Interval|Difference is meaningful; no true zero|
|Weight|Ratio|0 means no weight; all arithmetic allowed|

---

## 🔁 **Discrete vs Continuous Attributes**

|Category|Description|Examples|
|---|---|---|
|**Discrete**|Finite or countable set of values|Number of children, Ratings|
|**Continuous**|Infinite possible values (real numbers)|Height, Temperature, Salary|

---

## 🧩 Summary Table

|Type|Ordered?|Difference Meaningful?|True Zero?|Examples|
|---|---|---|---|---|
|Nominal|❌|❌|❌|Gender, Country|
|Ordinal|✅|❌|❌|Rank, Satisfaction Level|
|Interval|✅|✅|❌|Date, Temperature (°C)|
|Ratio|✅|✅|✅|Age, Salary, Distance|

## 📘 What is **Data Cleaning**?

**Data Cleaning** is the process of identifying and correcting (or removing) errors, inconsistencies, and inaccuracies in the dataset to improve its quality for analysis and mining.

Real-world data is often:

- **Incomplete** (missing values)
    
- **Noisy** (errors or outliers)
    
- **Inconsistent** (duplicate entries, conflicting values)
    

---

## ✅ **Objectives of Data Cleaning**

- Improve **data quality**
    
- Ensure **accurate** and **reliable** analysis
    
- Handle **missing**, **noisy**, and **inconsistent** data
    
- Make data **uniform and structured**
    

---

## 🔁 **Steps in Data Cleaning**

|Step|Description|Example|
|---|---|---|
|**1. Handle Missing Values**|Fill, ignore, or infer missing entries|Fill missing age with average|
|**2. Handle Noisy Data**|Remove or smooth out errors/outliers|Smooth salary values using binning|
|**3. Remove Duplicates**|Detect and delete identical or redundant records|Two entries with same ID and details|
|**4. Resolve Inconsistencies**|Standardize formats, units, or naming conventions|"M"/"Male", date format (MM/DD/YYYY vs DD/MM/YYYY)|
|**5. Data Validation**|Check if data follows expected rules or formats|Check if age > 0, email format valid|
|**6. Normalize/Standardize Data**|Convert data to consistent scale or range|Normalize income between 0 and 1|

---

## 🧪 Techniques for Handling Missing Data

|Method|Description|
|---|---|
|Ignore the record|Delete the record with missing values|
|Fill with default|Use predefined default value|
|Fill with mean/median|Use attribute's mean or median|
|Use predictive models|Predict missing value using classification or regression|

---

## 🧹 Example

Given this raw dataset:

|Name|Age|Gender|Salary|
|---|---|---|---|
|John|28|Male|45,000|
|Sarah||F|52,000|
|John|28|Male|45,000|
|Mike|150|Male|-|

After cleaning:

- Fill Sarah’s missing age with average
    
- Convert "F" to "Female"
    
- Remove duplicate John record
    
- Fix unrealistic age (150) or remove it
    
- Handle missing salary
    

---

## 🧩 Summary

Data Cleaning is **essential** before data mining. It ensures:

- **Accuracy**
    
- **Consistency**
    
- **Completeness**
    
- **Usability** of data


## 📘 What is **Data Integration**?

**Data Integration** is the process of combining data from multiple heterogeneous sources into a unified, consistent, and coherent data store, such as a data warehouse or a data lake.

It ensures that merged data from different sources can be used together for meaningful analysis.

---

## ✅ **Why is Data Integration Needed?**

In real-world scenarios:

- Data is spread across **multiple sources** (databases, files, APIs, cloud)
    
- Different sources may have **different formats**, **naming conventions**, or **data structures**
    

Data integration helps:

- Create a **unified view** of all data
    
- Enable **better analytics and reporting**
    
- Eliminate **data redundancy and inconsistency**
    

---

## 🔁 **Steps in Data Integration**

|Step|Description|Example|
|---|---|---|
|**1. Data Source Identification**|Identify relevant data sources|Customer data from CRM, sales, website|
|**2. Schema Integration**|Merge schemas (structures) from different sources|Combine "cust_id" from DB1 with "customerID" from DB2|
|**3. Data Value Conflicts Resolution**|Resolve unit mismatches or naming inconsistencies|Convert all currencies to USD, "M"/"Male"|
|**4. Redundancy Removal**|Eliminate duplicate or overlapping data|Same product listed twice under different IDs|
|**5. Data Transformation**|Normalize, aggregate, or reformat data|Change date format or scale income values|
|**6. Data Loading**|Store integrated data into data warehouse or storage|Load into central data warehouse|

After **integration**:

- Unified schema: `CustomerID`, `Name`, `Email`
    
- Remove redundancy
    
- Standardize formats
    

---

## ⚠️ **Challenges in Data Integration**

|Challenge|Description|
|---|---|
|**Schema mismatch**|Different names or types for same attribute|
|**Redundant data**|Same records stored in multiple places|
|**Conflicting data**|Inconsistent values (e.g., date format)|
|**Heterogeneous sources**|Data in files, relational DBs, NoSQL, etc.|

---

## 🧩 Summary

- Data Integration combines **heterogeneous data** sources into a **unified view**
    
- Steps include: **source identification**, **schema matching**, **conflict resolution**, **transformation**, and **loading**
    
- It's a **key step** in building data warehouses and ensuring clean, consolidated data for mining

## 📘 What is **Data Reduction**?

**Data Reduction** is the process of reducing the volume of data while maintaining the integrity and quality of the original data.  
The goal is to make data mining more **efficient**, **faster**, and **less resource-intensive** without losing important information.

---

## ✅ **Why is Data Reduction Needed?**

- Real-world datasets can be **huge** (terabytes or petabytes)
    
- Mining on large data is **slow**, **expensive**, and **memory-intensive**
    
- Many attributes may be **redundant** or **irrelevant**
    
- Reducing data helps in better **visualization** and **understanding**
    

---

## 🔁 **Types and Techniques of Data Reduction**

### 1. 🧮 **Data Cube Aggregation**

- Data is grouped and aggregated into **summary forms** (like totals or averages)
    
- Often used in **OLAP** and **data warehousing**
    
- Example: Sales data grouped by **month** instead of **day**
    

---

### 2. 📉 **Dimensionality Reduction**

- Reduces the number of **attributes** (columns/features)
    
- Helps remove **irrelevant or redundant** features
    

#### Techniques:
- **Principal Component Analysis (PCA)**
- **Feature selection** (e.g., Information Gain, Chi-square)
- **Attribute subset selection**

### 3. 📦 **Numerosity Reduction**
- Reduces **volume** of data (rows/records) by representing it in fewer forms
#### Approaches:

- **Parametric methods**: Model the data using functions (e.g., regression)
    
- **Non-parametric methods**: Histograms, clustering, sampling
    

---

### 4. 🎲 **Data Compression**

- Apply data encoding techniques to reduce storage size
    
- Lossless (no information loss) or Lossy (some loss allowed)
    

#### Methods:
- Huffman coding
- Run-length encoding    
- Wavelet transforms

### 5. 🎯 **Discretization and Binarization**

- Reduce the **range of numerical values**
    
- **Discretization**: Convert continuous data to intervals (e.g., age → young, middle, old)
    
- **Binarization**: Convert numerical attributes to 0/1 values
    

---

## 🧪 Example

|Original Dataset|After Reduction|
|---|---|
|10,000 daily sales records|12 monthly aggregated records|
|50 features (columns)|Top 10 selected using PCA|
|Continuous salary data|Discretized into 3 ranges|

---

## 🧩 Summary Table

|Technique|Goal|Example|
|---|---|---|
|Data Cube Aggregation|Summarization|Sum of sales per year|
|Dimensionality Reduction|Reduce features|Select 10 key features from 50|
|Numerosity Reduction|Reduce rows or instances|Sample 1,000 out of 1,00,000 rows|
|Data Compression|Reduce storage size|Huffman coding|
|Discretization|Group numeric values|Age → Child/Adult/Senior|

---

## ⚠️ Key Points for Exams:

- Data Reduction improves **efficiency** without losing essential information.
    
- Common methods: **Aggregation**, **Dimensionality Reduction**, **Compression**, **Discretization**
    
- Techniques like PCA, sampling, clustering, and encoding are widely used.

Here are **clear and concise exam notes** on **Data Transformation**, a key step in Data Preprocessing for Data Mining:

---

## 📘 What is **Data Transformation**?

**Data Transformation** is the process of converting data into a suitable format or structure for analysis.  
It includes operations that make raw data **more meaningful**, **consistent**, and **ready for mining**.

---

## ✅ **Why Data Transformation is Important**

- Real-world data is often inconsistent or incompatible
    
- Transformation helps:
    
    - Improve **data quality**
        
    - Enable **accurate analysis**
        
    - Ensure **uniform representation**
        
    - Facilitate **better mining performance**
``
## 🔁 **Common Data Transformation Techniques**

### 1. 🔢 **Normalization (Min-Max Scaling)**

- Rescales data to a specific range, usually [0, 1]
    
- Formula:
    ![](../../statics/Pasted%20image%2020250509173926.png)
- **Used in**: ML algorithms like KNN, SVM, Neural Nets
    

---

### 2. 📈 **Standardization (Z-score Normalization)**

- Rescales data based on mean and standard deviation
- ![](../../statics/Pasted%20image%2020250509173942.png)
- Mean = 0, Std Dev = 1
    
- **Used in**: Algorithms sensitive to variance

### 3. 🔣 **Encoding Categorical Data**

- Converts **text labels** to **numeric values**
    
- Types:
    
    - **Label Encoding**: Male → 1, Female → 0
        
    - **One-Hot Encoding**: Color → [Red, Green, Blue] → [1,0,0]
        

---

### 4. 🔀 **Attribute/Feature Construction**

- Creating **new attributes** from existing ones
    
- Example: `BMI = weight / (height)^2`

### 5. 🎚️ **Discretization**

- Convert **continuous attributes** into **categories**
    
- Example: Age → {Young, Middle-aged, Senior}
    
- Techniques: Binning, Histogram analysis, Clustering

### 6. 🔍 **Smoothing**

- Remove noise from data
    
- Techniques: Moving average, binning, regression
    

### 7. ↔️ **Aggregation**

- Combine data by summarizing (used in OLAP)
    
- Example: Daily sales → Monthly sales
    

---

### 8. 🔁 **Generalization**

- Replace low-level data with higher-level concepts
    
- Example: "Mumbai" → "India"
    

---

## 🧪 Example

|Original Data|Transformed Data|
|---|---|
|Age = 26|Age category = "Young"|
|Salary = 50,000|Normalized Salary = 0.5|
|Gender = "Male"|Gender = 1|
|Height, Weight|BMI = weight / height²|

---

## 🧩 Summary Table

|Technique|Purpose|
|---|---|
|Normalization|Scale data to [0,1]|
|Standardization|Zero mean, unit variance|
|Encoding|Convert categories to numbers|
|Discretization|Convert continuous to discrete|
|Feature Construction|Create new features|
|Smoothing|Reduce noise|
|Aggregation|Summarize data|
|Generalization|Replace specific with general|

---

## 📌 Exam Tips

- Be able to define **each transformation**
    
- Know **why and where** each is used
    
- Understand **formulas** (esp. for normalization & z-score)
    
- Give **real-life examples** (e.g., BMI, Age group)


## 📘 What is **Data Discretization**?

**Data Discretization** is the process of converting **continuous data** or numerical attributes into a set of **discrete intervals** or **categories**.

- It simplifies data and makes it **easier to analyze**
    
- Converts **infinite values** into **finite buckets**
    
- Commonly used in **classification** tasks and **decision trees**
    

---

## ✅ **Why is Discretization Needed?**

- Many data mining algorithms (e.g., **decision trees**, **Naive Bayes**) work better with **categorical attributes**
    
- Helps **reduce noise** and improves **interpretability**
    
- Makes data **compact and meaningful**
    

---

## 🔁 **Types of Discretization Techniques**

### 1. 📊 **Unsupervised Discretization** (no class label used)

#### a. **Equal-Width Binning (Equal Interval Size)**

- Divides the range of data into `k` equal-width intervals
    
- Example: Age 0–100 divided into 5 bins: 0–20, 21–40, ...
    

#### b. **Equal-Frequency Binning (Equal Depth)**

- Each bin contains **approximately the same number of data points**
    
- More adaptive to data distribution than equal-width
    

---

### 2. 🎯 **Supervised Discretization** (uses class label)

- Uses **target class information** to define bins
    
- Example: Discretize age based on probability of loan approval
    
- Techniques:
    
    - **Entropy-based methods** (like in decision trees)
        
    - **ChiMerge**: Merge intervals based on statistical significance (Chi-square test)
        

---

### 3. 📉 **Top-down (Splitting) vs Bottom-up (Merging)**

- **Top-down splitting** (e.g., recursive partitioning)
    
- **Bottom-up merging** (e.g., merging small intervals into larger ones)
    

---

### 4. 🧮 **Cluster-based Discretization**

- Use **clustering algorithms** (like k-means) to form groups
    
- Assign discrete labels based on cluster assignment
    

---

## 🧪 Example

**Original Age Data**:  
`18, 25, 32, 40, 50, 58, 70`

**After Equal-Width Discretization into 3 bins**:

- Bin 1: 18–35 → "Young"
    
- Bin 2: 36–52 → "Middle-aged"
    
- Bin 3: 53–70 → "Senior"
    

---

## 🧩 Summary Table

|Technique|Description|
|---|---|
|Equal-width binning|Fixed interval range|
|Equal-frequency binning|Same number of records per bin|
|Entropy-based (supervised)|Use class info to create best split|
|ChiMerge|Merge bins based on Chi-square test|
|Cluster-based|Use clustering algorithms for discretizing|

---

## ⚠️ Key Points for Exams

- Discretization is a **data reduction** and **transformation** technique
    
- Used for converting **continuous → categorical**
    
- Understand difference between **supervised** and **unsupervised** methods
    
- Know **examples and advantages** of each method
    
Here are **exam-ready notes** on **Measuring Similarity and Dissimilarity of Data** — a key topic in **Data Preprocessing** in Data Mining:

---

## 📘 What is Similarity and Dissimilarity?

- **Similarity**: A measure of how **alike** two data objects are
    
- **Dissimilarity**: A measure of how **different** two data objects are
    
- Both are used in **clustering**, **classification**, **recommendation systems**, etc.
    

---

## 🔁 General Properties

|Property|Similarity|Dissimilarity|
|---|---|---|
|Range|[0, 1] or unbounded|≥ 0|
|Higher value|More similar|More dissimilar|
|Lower value|Less similar|Less dissimilar (closer)|

---

## 🔢 Types of Data and Measures

### 1. **Numerical Data** (Continuous)

#### a. **Euclidean Distance**

![](../../statics/Pasted%20image%2020250510222004.png)

- Most common; used in K-Means, KNN
    
- Sensitive to scale
    

#### b. **Manhattan Distance** (City Block)

![](../../statics/Pasted%20image%2020250510222026.png)


- Useful in grid-like paths (e.g., chessboard, city streets)
    

#### c. **Minkowski Distance**

Generalized form:

![](../../statics/Pasted%20image%2020250510222045.png)

- p = 1 → Manhattan, p = 2 → Euclidean
    

---

### 2. **Binary Data**

- Two values: 0 and 1 (e.g., Male/Female, Yes/No)
    

#### a. **Simple Matching Coefficient (SMC)**
![](../../statics/Pasted%20image%2020250510222128.png)
- Suitable for **symmetric binary attributes**

#### b. **Jaccard Coefficient**

![](../../statics/Pasted%20image%2020250510222116.png)
- Ignores negative matches (0,0)
    
- Suitable for **asymmetric binary attributes**
    

---

### 3. **Categorical Data**

- Example: Color = {Red, Green, Blue}
    

#### a. **Simple Matching**

- Dissimilarity = Number of mismatches
    

---

### 4. **Ordinal Data**

- Attributes with a meaningful order (e.g., Low < Medium < High)
    

#### Steps:

1. Assign ranks (e.g., Low = 1, Medium = 2, High = 3)
    
2. Normalize ranks to [0,1]
    
3. Apply distance metric (e.g., Euclidean)
    

---

## 🔁 Similarity Measures

- **Cosine Similarity** (for text, high-dimensional data)

![](../../statics/Pasted%20image%2020250510222156.png)
- **Correlation Coefficient** (Pearson's r): Measures linear relationship

## 🧩 Summary Table

| Data Type     | Measure              | Use Case                          |
| ------------- | -------------------- | --------------------------------- |
| Numerical     | Euclidean, Manhattan | Clustering, Distance-based models |
| Binary        | Jaccard, SMC         | Presence/absence attributes       |
| Categorical   | Matching coefficient | Nominal data                      |
| Ordinal       | Ranked distance      | Survey data                       |
| Text / Sparse | Cosine similarity    | Document similarity, NLP          |

## 📊 **Data Visualization in Data Mining**

### ✅ What is Data Visualization?

**Data Visualization** is the graphical representation of data to help identify patterns, trends, outliers, and relationships in data. It makes complex data **easier to understand**, **interpret**, and **analyze**.

---

### ✅ Need for Data Visualization

- Helps in **understanding data distribution**
    
- Identifies **hidden patterns**, **trends**, and **outliers**
    
- Useful in **data pre-processing** (e.g., detecting missing values)
    
- Aids in **exploratory data analysis (EDA)**
    
- Helps evaluate results of **data mining models** (like clustering or classification)
    

---

### 📈 **Types of Visualization Techniques**

#### 1. **Univariate Visualization** (Single Attribute)

- **Histogram**
    
    - Shows distribution of a single numeric variable
        
    - **Example**: Frequency of different income ranges
        
- **Box Plot**
    A box plot (aka box and whisker plot) **uses boxes and lines to depict the distributions of one or more groups of numeric data**. Box limits indicate the range of the central 50% of the data, with a central line marking the median value.
    - Shows spread, median, outliers
- ![](../../statics/Pasted%20image%2020250510222447.png)
        
    - **Example**: Box plot of age in a dataset
        

---

#### 2. **Bivariate Visualization** (Two Attributes)

- **Scatter Plot**
    
    - Shows relationship between two numerical variables
        
    - **Example**: Age vs Salary
        
- **Line Chart**
    
    - Best for showing trends over time
        
    - **Example**: Monthly sales over a year
        
- **Bar Chart**
    
    - Compare categories with counts or averages
        
    - **Example**: Average marks per subject
        

---

#### 3. **Multivariate Visualization** (More than Two Attributes)

- **Heatmap**
    
    - Color-coded matrix to show correlation
        
    - **Example**: Correlation between exam scores of different subjects
        
- **Parallel Coordinates Plot**
    
    - For visualizing high-dimensional data
        
    - **Example**: Compare multiple features across different car models
        
- **Bubble Chart**
    
    - Like a scatter plot but with size representing a third variable
        
    - **Example**: Country-wise GDP (x), population (y), CO₂ emissions (size)
        

---

### 🧰 Tools for Visualization

- **Python Libraries**: Matplotlib, Seaborn, Plotly
    
- **BI Tools**: Tableau, Power BI
    
- **R Packages**: ggplot2
    
- **Excel**: Basic charts and dashboards
    

---

### 🔍 Example: Iris Dataset Visualization

- **Scatter Plot**: Petal length vs petal width by species
    
- **Histogram**: Distribution of sepal lengths
    
- **Box Plot**: Sepal width for each species
    

---

## ✅ Summary

|Technique|Best For|Example|
|---|---|---|
|Histogram|Distribution of numeric data|Income ranges|
|Box Plot|Summary + outliers|Student marks|
|Scatter Plot|Correlation between 2 variables|Height vs Weight|
|Heatmap|Correlation matrix|Sales vs Profits by Region|
|Line Chart|Trend over time|Stock price over months|
|Bar Chart|Category comparison|Product sales by category|
|Bubble Chart|3D numerical visualization|Country stats (GDP, population)|
## Database Warehouse 
### ✅ What is a Data Warehouse?

A **Data Warehouse (DW)** is a centralized repository that stores **integrated, historical, and subject-oriented data** from multiple sources. It is primarily used for **decision support and business intelligence (BI)** activities such as reporting, analysis, and data mining.

> 🔑 It enables **Online Analytical Processing (OLAP)**.

---

### ✅ Characteristics of a Data Warehouse

1. **Subject-Oriented**: Organized around key subjects (e.g., sales, customers).
    
2. **Integrated**: Combines data from different sources (databases, files).
    
3. **Time-Variant**: Maintains historical data for trends and forecasting.
    
4. **Non-Volatile**: Data is read-only; not updated by end-users.

## Data Warehouse architecture
![](../../statics/Pasted%20image%2020250510223046.png)

## 🏛️ **Data Warehouse Architecture – Notes**

### 🔄 **1. Data Sources**

- **Operational Data**: Internal systems such as sales, HR, CRM, etc.
    
- **External Data**: Outside sources like market data, social media, or third-party providers.
    

These sources provide the **raw data** to be loaded into the data warehouse.

---

### 🚚 **2. Load Manager (ETL Layer)**

- **Responsible for Extracting, Transforming, and Loading (ETL)** data into the warehouse.
    
- It ensures that data from both operational and external sources is:
    
    - Cleaned
        
    - Validated
        
    - Transformed into a consistent format
        

---

### 🏪 **3. Data Warehouse Storage**

Stores three main types of data:

- **Detailed Data**: Raw, granular data for deep analysis.
    
- **Summary Data**: Aggregated data for quick analysis and reporting.
    
- **Metadata**: Data about the data (source, structure, update times, usage, etc.)
    

---

### 🧠 **4. Warehouse Manager**

- Controls and manages the entire warehouse environment.
    
- Functions include:
    
    - Managing data updates
        
    - Index creation
        
    - Performance tuning
        
    - Archiving historical data
        
    - Data backup and recovery
        

---

### 💾 **5. Archive/Backup**

- Stores older data or backup copies of the warehouse.
    
- Ensures **data durability and recovery** in case of system failure.
    
- Converts raw **data into useful information** through organized storage.
    

---

### 🔎 **6. Query Manager**

- Handles all user queries and requests.
    
- Optimizes query performance using:
    
    - Caching
        
    - Query rewriting
        
    - Indexing
        

---

### 👤 **7. Users**

- **End-users, analysts, managers**, and decision-makers.
    
- Use tools like dashboards, OLAP systems, and reporting tools to interact with the data warehouse.
    

---

## 🔁 **Data Flow Summary**

1. Data is collected from **operational** and **external sources**.
    
2. Sent to **Load Manager** for ETL.
    
3. Stored in the **Warehouse** (detailed + summary + metadata).
    
4. Managed by the **Warehouse Manager** (including backup).
    
5. Accessed by users via the **Query Manager**.
    

---

## 🧾 **Conclusion**

This architecture ensures that data flows from raw format to clean, structured, and insightful information that helps users in decision-making.


## 📊 **OLAP – Online Analytical Processing**

### 🔍 **Definition**:

OLAP is a category of software tools that enables users to interactively analyze multidimensional data from multiple perspectives. It is used in **data mining, business intelligence, and decision support systems**.

---

### 🧱 **Key Features**:

- Supports **complex queries** quickly.
    
- Handles **large volumes** of historical data.
    
- Provides **multidimensional views** of data.
    
- Enables **drill-down**, **roll-up**, **slice**, **dice**, and **pivot** operations.
    

---

### 📦 **OLAP vs OLTP**:

|Feature|OLAP|OLTP|
|---|---|---|
|Purpose|Analysis & Reporting|Transaction Processing|
|Data Volume|Large (historical data)|Small (current data)|
|Operations|Complex read-heavy queries|Simple read-write operations|
|Speed|Optimized for reading|Optimized for writing|
|Data Normalization|Denormalized (star/snowflake)|Highly normalized|

---

### 🧱 **Types of OLAP**:

1. ### **MOLAP (Multidimensional OLAP)**:
    
    - Uses **multidimensional cubes**.
        
    - Data is stored in **array-based** structures.
        
    - Fast query performance.
        
    - Suitable for **smaller datasets**.
        
2. ### **ROLAP (Relational OLAP)**:
    
    - Uses **relational databases**.
        
    - Data is stored in **tables**.
        
    - Scalable and good for **large datasets**.
        
    - Slower than MOLAP.
        
3. ### **HOLAP (Hybrid OLAP)**:
    
    - Combines MOLAP and ROLAP.
        
    - Frequently accessed data in cubes (MOLAP), rest in relational (ROLAP).
        
    - Balance between storage and performance.
        

---

### 📌 **OLAP Operations**:

|Operation|Description|
|---|---|
|**Roll-up**|Aggregates data (e.g., month → quarter)|
|**Drill-down**|Provides more detail (e.g., year → month)|
|**Slice**|Selects one dimension (e.g., data for “March”)|
|**Dice**|Selects a sub-cube by multiple dimensions (e.g., March + TV + Delhi)|
|**Pivot**|Rotates the data view (rows ↔ columns)|

---

### 🏢 **Applications of OLAP**:

- Business reporting
    
- Financial forecasting
    
- Budgeting and planning
    
- Sales and marketing analysis
    
- Inventory and logistics

## OLAP vs OLTP

| Feature               | **OLTP (Online Transaction Processing)**         | **OLAP (Online Analytical Processing)**                     |
| --------------------- | ------------------------------------------------ | ----------------------------------------------------------- |
| **Purpose**           | Manage day-to-day transactions                   | Analyze historical data for decision making                 |
| **Operation Type**    | Insert, update, delete (write-heavy)             | Complex queries and data analysis (read-heavy)              |
| **Data Source**       | Current operational data                         | Historical, aggregated data from data warehouse             |
| **Data Volume**       | Smaller (real-time data)                         | Large volumes (historical + summarized data)                |
| **Database Design**   | Highly normalized (3NF) for efficiency           | Denormalized (star/snowflake schema) for fast query access  |
| **Users**             | Clerks, database admins, application users       | Executives, analysts, decision makers                       |
| **Speed**             | High for routine tasks                           | Optimized for read operations, slower than OLTP for updates |
| **Examples**          | Banking systems, ATM, ticket booking, e-commerce | Sales trend analysis, forecasting, business reporting       |
| **Backup & Recovery** | Essential                                        | Less critical, mostly read-only data                        |
| **Queries**           | Simple, pre-defined                              | Complex, multidimensional                                   |

## 📦 **Data Warehouse Implementation**

### 🔹 1. **Planning**

- **Define goals & objectives** of the data warehouse.
    
- Identify **key business processes** and **data sources**.
    
- Decide the **scope** (departmental, enterprise-wide, etc.).
    
- Create a **project roadmap** with timelines, milestones, and resource allocation.
    

---

### 🔹 2. **Requirements Gathering**

- Meet with stakeholders to understand:
    
    - **What data is needed?**
        
    - **Which KPIs and reports are required?**
        
- Document **functional and non-functional requirements**.
    

---

### 🔹 3. **Architecture Design**

- Choose architecture type:
    
    - **Top-Down (Inmon)**
        
    - **Bottom-Up (Kimball)**
        
    - **Hybrid**
        
- Define the layers:
    
    - **Data Source Layer**
        
    - **ETL Layer**
        
    - **Data Storage Layer**
        
    - **Presentation/Reporting Layer**
        

---

### 🔹 4. **Data Modeling**

- Choose schema design:
    
    - **Star Schema** – central fact table with dimension tables.
        
    - **Snowflake Schema** – normalized dimension tables.
        
    - **Fact Constellation** – multiple fact tables sharing dimensions.
        
- Design **metadata repository** to manage data definitions.
    

---

### 🔹 5. **ETL Process (Extract, Transform, Load)**

- **Extract** data from operational databases, flat files, APIs, etc.
    
- **Transform**: Clean, integrate, deduplicate, and reformat data.
    
- **Load**: Insert into data warehouse storage (incremental/full load).
    

---

### 🔹 6. **Data Storage & Indexing**

- Use **relational databases** or **cloud warehouses** (e.g., Snowflake, Redshift).
    
- Apply **indexing** and **partitioning** to improve query performance.
    
- Maintain **historical data** using slowly changing dimensions (SCDs).
    

---

### 🔹 7. **OLAP Cube Building**

- Create **data cubes** to support multidimensional analysis.
    
- Support operations like **slice, dice, drill-down, roll-up**.
    

---

### 🔹 8. **Front-End (BI) Tools Integration**

- Connect with tools like:
    
    - **Power BI**
        
    - **Tableau**
        
    - **Looker**
        
- Design **dashboards, reports, and visualizations**.
    

---

### 🔹 9. **Testing & Validation**

- **Data validation**: Check for accuracy and consistency.
    
- **Performance testing**: Ensure queries run efficiently.
    
- **User acceptance testing (UAT)**: Stakeholders validate outputs.
    

---

### 🔹 10. **Deployment**

- Go live in phases (e.g., pilot, full roll-out).
    
- Monitor **load performance**, **query times**, and **user feedback**.
    

---

### 🔹 11. **Maintenance & Monitoring**

- Schedule **regular updates** and **data refreshes**.
    
- Monitor for **ETL failures**, **query slowdowns**, and **storage issues**.
    
- Provide **user training and support**.
    

---

### ✅ Benefits of Proper Implementation:

- Faster decision-making.
    
- Centralized data access.
    
- Improved data quality and consistency.

## Data Cube
Grouping of data in a multidimensional matrix is called data cubes. In Dataware housing, we generally deal with various multidimensional data models as the data will be represented by multiple dimensions and multiple attributes. This multidimensional data is represented in the data cube as the cube represents a high-dimensional space. The Data cube pictorially shows how different attributes of data are arranged in the data model. Below is the diagram of a general data cube.
![](../../statics/Pasted%20image%2020250511062554.png)
### **Data cube classification:**

The data cube can be classified into two categories:

- **Multidimensional data cube:** It basically helps in storing large amounts of data by making use of a multi-dimensional array. It increases its efficiency by keeping an index of each dimension. Thus, dimensional is able to retrieve data fast.
- **Relational data cube:** It basically helps in storing large amounts of data by making use of relational tables. Each relational table displays the dimensions of the data cube. It is slower compared to a Multidimensional Data Cube.

### Data cube operation
- Dicing
- Slicing 
- Roll up
- pivot 
- Drill down

- **Multi-dimensional analysis**: Data cubes enable multi-dimensional analysis of business data, allowing users to view data from different perspectives and levels of detail.
- **Interactivity:** Data cubes provide interactive access to large amounts of data, allowing users to easily navigate and manipulate the data to support their analysis.
- **Speed and efficiency**: Data cubes are optimized for OLAP analysis, enabling fast and efficient querying and aggregation of data.
- **Data aggregation:** Data cubes support complex calculations and data aggregation, enabling users to quickly and easily summarize large amounts of data.
- **Improved decision-making:** Data cubes provide a clear and comprehensive view of business data, enabling improved decision-making and business intelligence.
- **Accessibility**: Data cubes can be accessed from a variety of devices and platforms, making it easy for users to access and analyze business data from anywhere.
- Helps in giving a summarised view of data.
- Data cubes store large data in a simple way.
- Data cube operation provides quick and better analysis,
- Improve performance of data.

### Disadvantages of data cube:

-  **Complexity**: OLAP systems can be complex to set up and maintain, requiring specialized technical expertise.
- **Data size limitations**: OLAP systems can struggle with very large data sets and may require extensive data aggregation or summarization.
- **Performance issues**: OLAP systems can be slow when dealing with large amounts of data, especially when running complex queries or calculations.
- **Data integrity:** Inconsistent data definitions and data quality issues can affect the accuracy of OLAP analysis.
- **Cost:** OLAP technology can be expensive, especially for enterprise-level solutions, due to the need for specialized hardware and software.

## Data warehouse models

Data modeling is the process of designing a visual representation of a system or database to establish how data will be stored, accessed, and managed. In the context of a data warehouse, data modeling involves defining how different data elements interact and how they are organized for efficient retrieval and analysis. The primary goal is to create a blueprint that guides the development of the data warehouse.

## Types of Data Models

Data modeling for data warehouses typically involves three main types of models:

![data_models](https://media.geeksforgeeks.org/wp-content/uploads/20250213155115526667/data_models-768.webp)

Types of Data Model

### ****1. Conceptual Data Model****

- ****Purpose:**** Provides a high-level overview of the business entities and their relationships without going into technical details.
- ****Components:**** Entities, relationships, and attributes.
- ****Example:**** A conceptual model might define entities like "Customer," "Product," and "Sales" and illustrate the relationships between them.

### ****2. Logical Data Model****

- ****Purpose:**** Represents the logical structure of the data, including the relationships between entities and the data types for each attribute, without considering physical storage.
- ****Components:**** Tables, columns, relationships, and constraints.
- ****Example:**** A logical model might define tables such as "Customer," "Product," and "Sales" with their respective columns like "CustomerID," "ProductID," and "SaleDate."

### ****3. Physical Data Model****

- ****Purpose:**** Specifies how the data will be physically stored in the database, including indexing, partitioning, and data storage mechanisms.
- ****Components:**** Tables, indexes, partitions, and storage settings.
- ****Example:**** A physical model might define storage settings for the "Sales" table, such as partitioning by date to improve query performance.

## Importance of Data Modeling in Data Warehouses

- ****Improved Data Quality:**** A well-structured data model helps ensure data consistency, accuracy, and reliability, which are critical for generating meaningful insights.
- ****Efficient Data Retrieval:**** By organizing data into logical structures, data modeling enables faster and more efficient data retrieval, which is essential for timely decision-making.
- ****Scalability:**** A robust data model allows for easy scaling of the data warehouse as the volume of data grows, ensuring that performance remains optimal.
- ****Reduced Redundancy:**** Proper data modeling helps eliminate data redundancy, reducing storage costs and simplifying data management.

## Key Data Modeling Techniques for Data Warehouses

### ****1. Star Schema****

The [star schema](https://www.geeksforgeeks.org/star-schema-in-data-warehouse-modeling/) is the simplest and most commonly used data warehouse schema. It consists of a central fact table surrounded by dimension tables.
![](../../statics/Pasted%20image%2020250511064124.png)

- ****Components:**** Fact tables, dimension tables.
- ****Advantages:****
    - Simple and easy to understand.
    - Efficient for querying large datasets.
- ****Use Case:**** Best suited for straightforward, query-intensive environments where speed is crucial.

### ****2. Snowflake Schema****

The [snowflake schema](https://www.geeksforgeeks.org/snowflake-schema-in-data-warehouse-model/) is a more complex version of the star schema where dimension tables are normalized into multiple related tables.

- ****Components:**** Fact tables, normalized dimension tables.
- ****Advantages:****
    - Reduces data redundancy.
    - Can improve query performance by reducing the size of dimension tables.
- ****Use Case:**** Ideal for data warehouses where space efficiency and data integrity are prioritized.

### ****3. Galaxy Schema (Fact Constellation):****

The galaxy schema, also known as a fact constellation, consists of multiple fact tables that share dimension tables. It's a more complex model used to represent multiple business processes.

- ****Components:**** Multiple fact tables, shared dimension tables.
- ****Advantages:****
    - Supports complex queries across multiple business processes.
    - Allows for more flexibility in data analysis.
- ****Use Case:**** Best for large, enterprise-level data warehouses with diverse business processes.

### 4. Normalized Data Model:**

This technique involves organizing the data into tables that reduce redundancy and dependency by splitting larger tables into smaller ones and linking them via relationships.

- ****Components:**** [Normalized](https://www.geeksforgeeks.org/introduction-of-database-normalization/) tables, relationships.
- ****Advantages:****
    - Eliminates data redundancy.
    - Improves data integrity.
- ****Use Case:**** Suitable for environments where data consistency and integrity are more critical than query performance.

### **5. Denormalized Data Model:

[Denormalization](https://www.geeksforgeeks.org/denormalization-in-databases/) is the process of combining normalized tables into larger tables to reduce the complexity of queries and improve performance.

- ****Components:**** Denormalized tables, fewer relationships.
- ****Advantages:****
    - Faster query performance.
    - Simplified query design.
- ****Use Case:**** Ideal for data warehouses where query speed is more important than storage efficiency.


## Concept Hierarchy 
In data mining, the concept of a concept hierarchy refers to the organization of data into a tree-like structure, where each level of the hierarchy represents a concept that is more general than the level below it. This hierarchical organization of data allows for more efficient and effective data analysis, as well as the ability to drill down to more specific levels of detail when needed. The concept of hierarchy is used to organize and classify data in a way that makes it more understandable and easier to analyze. The main idea behind the concept of hierarchy is that the same data can have different levels of granularity or levels of detail and that by organizing the data in a hierarchical fashion, it is easier to understand and perform analysis.

## 📚 **Types of Concept Hierarchies (Simplified)**

1.  **Schema Hierarchy**
    
    - Organizes database components like **tables, attributes, and relationships**.
        
    - Helps group similar data in a logical way.
        
    - 📌 **Used in**: Data warehousing to integrate data from different sources.
        
2.  **Set-Grouping Hierarchy**
    
    - Based on **set theory** – groups data into sets and subsets.
        
    - Helps in **cleaning, preprocessing, and integrating** data.
        
    - 📌 **Used to**: Remove noise/outliers and combine data from many sources.
        
3.  **Operation-Derived Hierarchy**
    
    - Created by applying **operations** (like aggregation or normalization) to data.
        
    - Each level shows a more **general** version of the previous level.
        
    - 📌 **Used in**: Data mining for **clustering** or **dimensionality reduction**.
        
4.  **Rule-Based Hierarchy**
    
    - Built by applying **rules or conditions** to data.
        
    - Helps **classify** or **make decisions** based on data characteristics.
        
    - 📌 **Used in**: Classification, decision-making, and pattern discovery.
        

---

✅ **Summary**:

|Type|Key Use|Example Use Case|
|---|---|---|
|Schema Hierarchy|Organizing schema/data structure|Grouping similar tables|
|Set-Grouping Hierarchy|Cleaning and combining data|Removing outliers|
|Operation-Derived|Generalizing data via operations|Aggregating sales data|
|Rule-Based|Classifying using conditions/rules|Decision trees, classification|

## Multidimensional data models

A Multidimensional Data Model is defined as a model that allows data to be organized and viewed in multiple dimensions, such as product, time and location

## Features of multidimensional data models

- ****Measures:**** Measures are numerical data that can be analyzed and compared, such as sales or revenue. They are typically stored in fact tables in a multidimensional data model.
- ****Dimensions:**** Dimensions are attributes that describe the measures, such as time, location, or product. They are typically stored in dimension tables in a multidimensional data model.
- ****Cubes:**** Cubes are structures that represent the multidimensional relationships between measures and dimensions in a data model. They provide a fast and efficient way to retrieve and analyze data.
- ****Aggregation:**** Aggregation is the process of summarizing data across dimensions and levels of detail. This is a key feature of multidimensional data models, as it enables users to quickly analyze data at different levels of granularity.
- ****Drill-down and roll-up:**** Drill-down is the process of moving from a higher-level summary of data to a lower level of detail, while roll-up is the opposite process of moving from a lower-level detail to a higher-level summary. These features enable users to explore data in greater detail and gain insights into the underlying patterns.
- ****Hierarchies:**** Hierarchies are a way of organizing dimensions into levels of detail. For example, a time dimension might be organized into years, quarters, months, and days. Hierarchies provide a way to navigate the data and perform drill-down and roll-up operations.
- ****OLAP (Online Analytical Processing):**** OLAP is a type of multidimensional data model that supports fast and efficient querying of large datasets. OLAP systems are designed to handle complex queries and provide fast response times.

## Advantages of Multi Dimensional Data Model 

The following are the advantages of a multi-dimensional data model :

- A multi-dimensional data model is easy to handle.
- It is easy to maintain.
- Its performance is better than that of normal databases (e.g. relational databases).
- The representation of data is better than traditional databases. That is because the multi-dimensional databases are multi-viewed and carry different types of factors.
- It is workable on complex systems and applications, contrary to the simple one-dimensional database systems.
- The compatibility in this type of database is an upliftment for projects having lower bandwidth for maintenance staff.

## Disadvantages of Multi Dimensional Data Model

The following are the disadvantages of a Multi Dimensional Data Model :

- The multi-dimensional Data Model is slightly complicated in nature and it requires professionals to recognize and examine the data in the database.
- During the work of a Multi-Dimensional Data Model, when the system caches, there is a great effect on the working of the system.
- It is complicated in nature due to which the databases are generally dynamic in design.
- The path to achieving the end product is complicated most of the time.
- As the Multi Dimensional Data Model has complicated systems, databases have a large number of databases due to which the system is very insecure when there is a security break
- 

# UNIT 3
##  Frequent Patterns Mining

## 🔍 **Frequent Patterns in Data Mining**

Frequent patterns are patterns that **occur frequently** in a dataset. They help discover **regularities or trends** in data.

---

### ✅ 1. **Frequent Itemsets**

- **Definition**: A _frequent itemset_ is a set of items that appear **together** in many transactions.
    
- **Used in**: Association rule mining and market basket analysis.
    

#### Example:

Transaction Database:

```
T1: {Milk, Bread}
T2: {Milk, Bread, Butter}
T3: {Milk, Butter}
T4: {Bread, Butter}
T5: {Milk, Bread, Butter}
```

- Frequent Itemsets (with minimum support = 60%):
    
    - {Milk} → appears in 4/5 transactions
        
    - {Bread} → appears in 4/5 transactions
        
    - {Milk, Bread} → appears in 3/5 transactions
        
    - {Milk, Bread, Butter} → appears in 2/5 → NOT frequent (if support < 60%)
        

---

### 🔄 2. **Sequential Patterns**

- **Definition**: Patterns where **order of items matters**.
    
- Looks at sequences of itemsets over **time**.
    

#### Example:

Customer Purchase Sequences:

```
Customer 1: <{Milk} → {Bread}>
Customer 2: <{Milk} → {Bread, Butter}>
Customer 3: <{Milk, Butter} → {Bread}>
```

- Pattern: `<{Milk} → {Bread}>` is frequent if it appears in 2 or more sequences.
    
- Meaning: People **first buy Milk**, then **later buy Bread**.
    

---

### 🆚 Difference Between Itemset and Sequential Patterns

|Feature|Frequent Itemset|Sequential Pattern|
|---|---|---|
|Order matters?|❌ No|✅ Yes|
|Time-based sequence?|❌ No|✅ Yes|
|Example|{Milk, Bread}|<{Milk} → {Bread}>|
|Application|Market basket, recommender|Web usage mining, customer behavior|

---

Let me know if you’d like visual diagrams or sample problems!


## FP-Growth
![](../../statics/Pasted%20image%2020250511071519.png)
https://www.geeksforgeeks.org/frequent-pattern-growth-algorithm/

## Apriori

Apriori is a **frequent pattern mining algorithm** used to find **frequent itemsets** and generate **association rules** from transactional datasets.  
It is based on the **Apriori principle**:

> _If an itemset is frequent, all of its subsets must also be frequent._
## 📘 Apriori Algorithm

### ✅ **Steps of Apriori Algorithm**:

1. **Set minimum support and confidence thresholds**
    
    - Determine the minimum support count and confidence for frequent itemsets and rules.
        
2. **Generate frequent 1-itemsets**
    
    - Count the support of each item in the transaction database.
        
    - Retain only those meeting the minimum support.
        
3. **Generate candidate k-itemsets (k ≥ 2)**
    
    - Use the **frequent (k−1)-itemsets** to generate candidate k-itemsets (self-join).
        
    - Apply the **Apriori property** to remove candidates that have any infrequent (k−1)-subset.
        
4. **Scan the database**
    
    - Count the support for each candidate itemset.
        
    - Retain those that meet the support threshold.
        
5. **Repeat steps 3–4**
    
    - Continue until no new frequent itemsets are found.
        
6. **Generate strong association rules**
    
    - From frequent itemsets, generate rules with confidence ≥ threshold.
        

---

### 👍 **Advantages of Apriori**:

- Simple and intuitive to understand and implement.
    
- Effectively reduces the search space using the Apriori property.
    
- Works well for small and sparse datasets.
    
- Can handle large itemsets and produce all strong association rules.
    

---

### 👎 **Disadvantages of Apriori**:

- Requires **multiple passes** over the dataset (high I/O cost).
    
- **Generates a large number of candidate sets**, especially when the dataset is large.
    
- **Performance decreases** as the number of items increases.
    
- Less efficient for dense datasets or long frequent patterns.
    
- High memory and computation cost due to candidate generation and pruning.
    

### 🚀 Improving Efficiency of Apriori Algorithm

### 1. **Hash-based Itemset Counting**

- **Concept**: Use a hash table to reduce the number of candidate k-itemsets.
    
- **How**: During candidate generation, hash itemsets into buckets and count frequencies. Discard those in low-support buckets early.
    
- **Benefit**: Reduces the number of candidate itemsets scanned in later steps.
    

---

### 2. **Transaction Reduction**
- **Concept**: Remove transactions that don’t contain any frequent k-itemsets.
- **How**: After each pass, eliminate transactions that do not help in finding further itemsets.
- **Benefit**: Reduces the database size for the next pass.

### 3. **Partitioning**

- **Concept**: Divide the database into partitions and find local frequent itemsets.
    
- **How**: Only itemsets that are frequent in at least one partition are tested globally.
    
- **Benefit**: Scans the database only twice and reduces candidate sets.
    

---

### 4. **Sampling**

- **Concept**: Run Apriori on a small random sample of the database.
    
- **How**: Estimate frequent itemsets and then verify them in the full database.
    
- **Benefit**: Fast initial mining, though final step still needs full database scan.
    

---

### 5. **Dynamic Itemset Counting (DIC)**

- **Concept**: Add new candidate itemsets dynamically as the database is scanned.
    
- **How**: Unlike Apriori, doesn’t wait for one full pass to complete before generating candidates.
    
- **Benefit**: Reduces number of full database scans.
    

---

### 6. **Vertical Data Format (ECLAT approach)**

- **Concept**: Store itemsets with their transaction IDs (TIDs).
    
- **How**: Intersect TID-lists to compute support, instead of scanning transactions.
    
- **Benefit**: Faster computation using set intersection.
    

---

### 7. **Using Frequent Pattern Tree (FP-Tree)**

- **Concept**: Instead of candidate generation, compress data into a tree and mine directly.
    
- **Benefit**: Eliminates costly candidate generation, much faster for dense datasets.



## FP Growth

### ➤ What is FP-Tree?

The FP-Tree (Frequent Pattern Tree) is a compact data structure that stores crucial information about frequent patterns in a dataset. It helps eliminate the costly candidate generation step used in the Apriori algorithm.

### ➤ Key Features:

- It is a prefix-tree that represents frequent itemsets.
    
- It allows the mining of frequent patterns without generating candidate itemsets.
    
- It maintains the association between items and their frequency.
    

### ➤ Structure of FP-Tree:

1. **Root node**: Always labeled as "null" or empty.
    
2. **Item nodes**: Represent items in transactions.
    
3. **Item frequency**: Each node also holds a count of how often the item appears in that path.
    
4. **Header Table**: Keeps track of each item and links to its occurrences in the tree.
    

### ➤ FP-Tree Construction Process:

1. Scan the database once to find frequent items.
    
2. Sort items in each transaction by frequency (descending order).
    
3. Build the tree by inserting each sorted transaction, merging common prefixes, and updating counts.
    
4. Maintain node links between identical items using the header table.

### ➤ What is FP-Growth?

FP-Growth (Frequent Pattern Growth) is a pattern mining algorithm that uses the FP-Tree to find frequent itemsets efficiently. It works by recursively extracting frequent patterns from the tree without generating candidate itemsets.

The **FP-Growth algorithm** mainly consists of these **three core steps**, each playing a critical role in mining frequent itemsets efficiently from the FP-Tree. Below is a detailed theoretical explanation of each step:

## 🔶 1. Conditional Pattern Base (CPB)

### ❖ Definition:

A **Conditional Pattern Base** is a collection of **prefix paths** in the FP-Tree that co-occur with a given frequent item. It serves as the "sub-database" used to create the conditional FP-Tree.

### ❖ Purpose:

To isolate the subset of the dataset relevant to a specific item, allowing us to focus only on the patterns that contain that item.

### ❖ How it works:

- For a given item **‘A’**, traverse the FP-Tree using the header table to collect all paths leading to ‘A’.
    
- Each path, along with its frequency count, is added to the conditional pattern base of **‘A’**.
    
- This base is then used to construct the conditional FP-Tree.
    

---

## 🔷 2. Conditional FP-Tree

### ❖ Definition:

The **Conditional FP-Tree** is a smaller, item-specific FP-Tree built from the Conditional Pattern Base of an item. It represents the projected database for that item.

### ❖ Purpose:

To recursively mine frequent patterns that include the base item.

### ❖ How it works:

- From the conditional pattern base, count the frequencies of items.
    
- Discard infrequent items.
    
- Build the conditional FP-Tree by inserting paths from the CPB (just like a normal FP-Tree but restricted to that item).
    

### ❖ Note:

If a conditional FP-Tree contains only one path, we can directly generate all combinations of items in that path with the base item (pattern fragment growth).

---

## 🔸 3. Frequent Pattern Generation

### ❖ Definition:

This is the final step where **frequent itemsets** are generated by combining the base item with the frequent patterns derived from its conditional FP-Tree.

### ❖ How it works:

- Starting with a single frequent item, generate patterns by **joining it** with all patterns recursively mined from its conditional FP-Tree.
    
- This process is repeated for **each item** in the header table, moving bottom-up (least frequent to most frequent).
    

### ❖ Result:

A complete set of frequent itemsets without generating candidate sets, which is the major advantage over Apriori.

---

### ✅ Summary

|Step|Purpose|
|---|---|
|Conditional Pattern Base|Extract relevant prefix paths for a target item|
|Conditional FP-Tree|Build a tree specific to the target item|
|Frequent Pattern Generation|Recursively mine patterns by combining base items|

Would you like a flow diagram or pseudocode of this process next?